{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa871d8-f7f3-4be7-a03b-95874f8815b5",
   "metadata": {},
   "source": [
    "https://youtu.be/g-41MbfW4yw?t=513 \n",
    "To get the attention scores, we scale the query-key dot product by the dim of the key (== dim of the query)\n",
    "\n",
    "dim_of_key = key.size(-1)                                                       \n",
    "attention_scores = torch.bmm(query, key.transpose(-2,-1))/sqrt(dim_of_key) \n",
    "\n",
    "where key (and query) are dim (batch_size, seq_len, embed_size//#heads)\n",
    "\n",
    "At this point, in this attention head, we have pushed the full embedding through the attention head-specific Wq/Wk/Wv reduce\n",
    "embed_size to embed_size//#heads \n",
    "\n",
    "In the multi-head attenttion layer, we give each head the full embedding. The head then \"shrinks\" it using its Wq/Wk/Wv and \n",
    "applies attention. The multi-head attention layer then reconstructs the head outputs to something of size embed_size\n",
    "\n",
    "Back to scaled dot product:\n",
    "\n",
    "We do this to \"preserve variance\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deeffb5-06f1-4f51-9228-bc269a7d45f6",
   "metadata": {},
   "source": [
    "Softmax does not like very large or very small numbers, will go to 1 or 0 respectively - test this\n",
    "What we want is a nice distribution in what we feed into softmax\n",
    "We want to preserve the variance (nice distribution) which is what scaling does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "938519c4-00af-4881-a638-3c8d6b8a87ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9631)\n",
      "tensor(1.0047)\n",
      "tensor(24.2776)\n",
      "torch.Size([125, 125])\n",
      "tensor(0.9711)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dim = 25\n",
    "a = torch.randn(5*dim,dim) # randn fills with random numbers but variance is 1 (and mean is 0)\n",
    "b = torch.randn(dim,5*dim)\n",
    "ab = a@b\n",
    "print(a.var())\n",
    "print(b.var())\n",
    "print(ab.var())\n",
    "print(ab.shape)\n",
    "ab = ab / ((dim)**.5)\n",
    "print(ab.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d64f09-9e73-457d-a1e0-f14fb2fcf95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takeway is that our variance just blew up, which is what we don't want for stable learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb6a6ae2-1b72-40d0-8d94-59a5fbf91ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9807)\n",
      "tensor(1.0259)\n",
      "tensor(126.3244)\n",
      "torch.Size([25, 25])\n",
      "tensor(1.0106)\n"
     ]
    }
   ],
   "source": [
    "dim = 25\n",
    "a = torch.randn(dim,5*dim) # randn fills with random numbers but variance is 1 (and mean is 0)\n",
    "b = torch.randn(5*dim,dim)\n",
    "ab = a@b\n",
    "print(a.var())\n",
    "print(b.var())\n",
    "print(ab.var())\n",
    "print(ab.shape)\n",
    "ab = ab / ((5*dim)**.5)\n",
    "print(ab.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92ec50-ccd5-4328-99e4-c40d5656afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So it's the \"inner\" dimension on which we multiply !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "903a5eb7-2cb7-42a6-b870-e98836a0ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(54.8608)\n",
      "tensor(56.0311)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(123.1824)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a.norm())\n",
    "print(b.norm())\n",
    "ab.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d76771-179b-4d23-9b46-d381e21607d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the norm is increase, but that does not matter, because softmax only cares about variance and not norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c025a0f-5b55-4229-be86-6ee0883cae03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0089e-43, 2.7465e-43, 8.1940e-40, 1.0000e+00])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = torch.tensor([1.,2.,10.,100.]) # This is a very high variance\n",
    "torch.nn.functional.softmax(my_data, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f3043-5beb-4ed6-83fd-641f756be0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This very high variance gives us a poor softmax result: the 100 is 1, all the rest is squashed to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cb39668-f2e5-42fc-9ff2-1bc004d98fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2586, 0.2340, 0.1916, 0.3158])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = torch.tensor([1.,.9,.7,1.2]) # This is a low variance\n",
    "torch.nn.functional.softmax(my_data, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d029937-ee46-49ad-a60e-6b6139306620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2586, 0.2340, 0.1916, 0.3158])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we get usable softmax output\n",
    "# New we increase the norm by adding 1000 to each element\n",
    "\n",
    "my_data += 1000\n",
    "torch.nn.functional.softmax(my_data, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26d9aa-3460-4af5-9d51-112dfc0b6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the same softmax result!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

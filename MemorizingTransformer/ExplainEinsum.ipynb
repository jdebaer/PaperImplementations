{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e922b0fb-dc74-4425-9080-d5e60fd56797",
   "metadata": {},
   "source": [
    "Memory expects incoming queries to be (b_s, seq_len, embed_dim)     (note that queries and in fact keys, check this)\n",
    "\n",
    "Memory returns results in (b_s, seq_len, top_k, 2, embed_dim)\n",
    "\n",
    "In the video, the embed_dim is cut into pieces and each attn head gets a piece, so what needs to happen with the results is this:\n",
    "\n",
    "1. Split out keys and values:\n",
    "    keys: (b_s, seq_len, top_k, embed_dim)\n",
    "    values: (b_s, seq_len, top_k, embed_dim)\n",
    "2. Cut both in pieces to prep feeding to the attn heads:\n",
    "    keys: (b_s, seq_len, top_k, heads, head_dim)\n",
    "    values: (b_s, seq_len, top_k, heads, head_dim)\n",
    "3. Now we need to pull the head dimension forward as we're going to feed it all into each head (except the batch):\n",
    "    queries: (b_s, heads, seq_len, head_dim)\n",
    "    keys: (b_s, heads, seq_len, head_dim, top_k) (we also need to pull head_dim forward)\n",
    "    values: (b_s, heads, seq_len, top_k, head_dim)\n",
    "    # This means we're also chopping up the memories and feeding the pieces to the attn heads\n",
    "\n",
    "4. Then we also want to do self attention in each head with the pieces, where\n",
    "    qk is obtained by (b_s, heads, seq_len, head_dim) @ (b_s, heads, seq_len, head_dim, top_k) -> (b_s, heads, seq_len, top_k)\n",
    "\n",
    "[1, 2, 3]  @  [2,4] --> [2 + 8 + 18, 4 + 10 + 21] -> basically you get an attention score for each top_k, for each token in the sequence     \n",
    "              [4,5]\n",
    "              [6,7]\n",
    "\n",
    "    then we must also multiply that with the value:\n",
    "\n",
    "    (b_s, heads, seq_len, top_k) @ (b_s, heads, seq_len, top_k, head_dim) -> \n",
    "\n",
    "   [2, 3] @ [1, 2, 3, 4, ...]  --> [2 + 15, 4 + 18, ... ] , we essentially add the pieces up \n",
    "            [5, 6, 7, 8, ...] \n",
    "\n",
    "                                                                                   \n",
    "In our implemtation however, we feed each attention head the complete embed_dim so we don't need to do this, we can reuse the existing logic.\n",
    "\n",
    "However, below is how it's done in the videa with einsum (without top_k).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de2bf13e-6447-447b-bd55-091dfcbd9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is challenge setting:\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "b_s = 8\n",
    "seq_len = 512\n",
    "num_attn_heads = 4\n",
    "head_dim = 4\n",
    "\n",
    "queries = torch.randn(b_s, seq_len, num_attn_heads * head_dim)\n",
    "keys = torch.randn(b_s, seq_len, num_attn_heads * head_dim)\n",
    "\n",
    "queries = rearrange(queries, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=num_attn_heads)\n",
    "keys = rearrange(keys, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=num_attn_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "450165d0-a51a-4222-b987-ea9acc9e5fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3., 3.]])\n",
      "tensor([[2.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n",
      "tensor([[1.6491, 0.7200],\n",
      "        [0.7200, 0.8063]])\n",
      "tensor([[0.5253, 0.9324, 0.2290],\n",
      "        [0.0776, 0.8039, 0.3638],\n",
      "        [0.7593, 0.1732, 0.5688]])\n",
      "tensor([0.5253, 0.8039, 0.5688])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.8979)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code examples from https://www.youtube.com/watch?v=pkVwUVEHmfI\n",
    "\n",
    "# Rough rules\n",
    "#\n",
    "# - Repeated indices tell einsum to multiply on those dimensions\n",
    "# - Differing indices define the shape of the output\n",
    "# - Indices that are not present in the output means that summation happens over that dimension\n",
    "# - Output indices (dimensions) can be put in any order\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.rand((2,3))\n",
    "\n",
    "# Permutation of tensors\n",
    "# ----------------------\n",
    "\n",
    "# Note: transpose is flipping two dimensions, so it's a special case of permutation. So the below are equivalent:\n",
    "\n",
    "batch_size, seq_len = 3, 5\n",
    "a = torch.zeros((batch_size, seq_len))\n",
    "a[0] = 1\n",
    "a[1] = 2\n",
    "a[2] = 3\n",
    "\n",
    "print(a)\n",
    "\n",
    "a1 = a.transpose(1,0)\n",
    "a2 = a.permute(1,0)\n",
    "\n",
    "# Note on note: always rememeber that view() and reshape() do something fundamentally different, so the below gives a different result\n",
    "\n",
    "a3 = a.view(seq_len, -1)\n",
    "a4 = a.reshape(seq_len, -1)\n",
    "\n",
    "torch.einsum('rc->cr', a) # This is equivalent to the transpose/permute above\n",
    "\n",
    "# Summation RULE: THE DIMENSION THAT'S OMITTED IN THE OUTPUT IS THE ONE OVER WHICH THE SUMMATION HAPPENS\n",
    "# ----------------------\n",
    "\n",
    "# 1. Sum up the whole matrix\n",
    "\n",
    "torch.einsum('rc->', a)\n",
    "\n",
    "# 2. Sum up the rows\n",
    "\n",
    "torch.einsum('rc->r',a)\n",
    "\n",
    "# Matrix - vector multiplication (transformation)\n",
    "# ----------------------\n",
    "\n",
    "v = torch.rand((3,1))\n",
    "v[0] = 2\n",
    "v[1] = 2\n",
    "v[2] = 2\n",
    "print(v)\n",
    "m = torch.rand((3,3))\n",
    "m[0] = 1\n",
    "m[1] = 2\n",
    "m[2] = 3\n",
    "print(m)\n",
    "\n",
    "# Normally you need to transpose v, but with einsum you can specify the dimension on which the mm must happen so no need:\n",
    "\n",
    "result = torch.einsum('r v, r m -> v m', v, m)\n",
    "result\n",
    "\n",
    "# Matrix - matrix multiplication\n",
    "# ----------------------\n",
    "\n",
    "print(x.mm(x.t()))   # 2x3 @ 3x2 -> 2x2\n",
    "\n",
    "# Again with einsum you don't have to transpose, just specify the @ dimesion:\n",
    "\n",
    "torch.einsum('r c, d c -> r d', x,x)\n",
    "\n",
    "# You can also single out rows in the input, here we are taking the dot product of the first row with itself:\n",
    "\n",
    "torch.einsum('i,i->', x[0], x[0])\n",
    "\n",
    "# Hadamard product / element-wise multiplication (so no summing)\n",
    "# ----------------------\n",
    "\n",
    "torch.einsum('ij, ij -> ij', x, x)\n",
    "\n",
    "# Outer product\n",
    "# ----------------------\n",
    "\n",
    "a = torch.rand((3))\n",
    "b = torch.rand((5)) # -> result must be 3x5 matrix\n",
    "\n",
    "torch.einsum('r,c->rc', a, b)\n",
    "\n",
    "# Batch matrix multiplication\n",
    "# ----------------------\n",
    "\n",
    "a = torch.rand((3,2,5))\n",
    "b = torch.rand((3,5,3)) # -> we want to do 3 bmm's of 2x5 @ 5x3 -> 2x3\n",
    "\n",
    "torch.einsum('bij, bjk -> bik', a, b)\n",
    "\n",
    "# Matrix diagonal\n",
    "# ----------------------\n",
    "\n",
    "m = torch.rand((3,3))\n",
    "print(m)\n",
    "print(torch.einsum('ii->i', m)) # Map 0,0 to 0, 1,1 to 1, etc.\n",
    "\n",
    "\n",
    "# Matrix trace ( == sum over the diagonal)\n",
    "# ----------------------\n",
    "\n",
    "torch.einsum('ii->', m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e304f93d-6e67-4a1c-9a80-f17b05f05306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 6, 4, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Back to coding a paper \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "# Rearrange is something that's not part of standard einsum. Esssentially this allows to reshape while transposing:\n",
    "\n",
    "# torch.rand vs torch.randn:\n",
    "# torch.randn generates numbers from a normal distribution with a mean of 0 and a standard deviation of 1\n",
    "# torch.rand generates numbers from a uniform distribution between 0 and 1.\n",
    "# For these you *can* directly provide the dimensions as parmaters, or as a tuple.\n",
    "\n",
    "# Similar with np.random.rand and np.random.randn but here you immediately put the dimensions (only parameters)\n",
    "\n",
    "x = torch.randn((1,2,3))\n",
    "y = torch.randn(1,2,3)\n",
    "\n",
    "#x1 = np.random.randn((1,2,3)) <<< not supported\n",
    "y1 = np.random.randn(1,2,3)\n",
    "\n",
    "x = torch.randn(24, 10, 15)\n",
    "\n",
    "rearrange(x, '(a b) c (d e) -> (e c) a b d', a=6, e=5).shape # b and e are product-wise inferred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ffce439-4292-45ec-8cc3-ea2dfc4a10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is challenge setting:\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "b_s = 8\n",
    "seq_len = 512\n",
    "num_attn_heads = 4\n",
    "head_dim = 4\n",
    "\n",
    "queries = torch.randn(b_s, seq_len, num_attn_heads * head_dim)\n",
    "keys = torch.randn(b_s, seq_len, num_attn_heads * head_dim)\n",
    "\n",
    "queries = rearrange(queries, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=num_attn_heads)\n",
    "keys = rearrange(keys, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=num_attn_heads)\n",
    "\n",
    "# Now that we know all of the above, let's continue with the challenge\n",
    "\n",
    "# For the q k dot product in each attn head, we multiply over the head_dim (d) for each element in the sequence, so that \n",
    "# we get a seq_len x seq_len result matrix - this is standard attention\n",
    "\n",
    "queries_keys = einsum(queries, keys, 'b h s1 d, b h s2 d -> b h s1 s2')\n",
    "\n",
    "queries_keys = queries_keys * (head_dim ** -0.5) # This happens across the heads, which works because all the head_dim's are the same\n",
    "\n",
    "# 41:30 -> implement the forward() here as an example\n",
    "\n",
    "# Here is a forward() at the level of the multi-head attention class, implemented with einsum:\n",
    "\n",
    "def forward(self, input_data):  # What comes in in (b_s, seq_len, embed_dim)\n",
    "\n",
    "    b_s, seq_len = input_data.shape[:2]  # All dims except dim #2 (the third one)\n",
    "\n",
    "    # In the vid implementation the Wq/k/v transformations are done in the multi-head attention class, not in the attn heads\n",
    "\n",
    "    queries = self.Wq(input_data)     \n",
    "    keys = self.Wk(input_data)\n",
    "    values = self.Wv(input_data)\n",
    "    \n",
    "    # Now we split and multple with einsum (copy from the above, just added self. ...)\n",
    "    queries = rearrange(queries, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "    keys = rearrange(keys, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "    \n",
    "    values = rearrange(values, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "    \n",
    "    # Calculate attention scores for all heads and scale (all copied from above)\n",
    "\n",
    "    queries_keys = einsum(queries, keys, 'b h s1 d, b h s2 d -> b h s1 s2')\n",
    "\n",
    "    queries_keys = queries_keys * (head_dim ** -0.5)\n",
    "\n",
    "    # Here is where you do queries_keys = rel_pos_values + queries_keys -> DO THIS BEFORE MASKING FOR SURE !!!!!!\n",
    "\n",
    "    # Causal masking - at this point queries_keys is (b_s, num_attn_heads, seq_len, seq_len)\n",
    "\n",
    "    # Vid takes last two dims separately - should they not always be the same? We write assert here to check it. Vid code below\n",
    "    # in case this assert ever fires.\n",
    "\n",
    "    i, j = queries_keys.shape[-2:]\n",
    "    assert i == j, \"Attention scores in attention head are not stored in a square matrix!\"\n",
    "    # mask = torch.ones((i,j), dtype=torch.bool).triu(j-i+1)\n",
    "    # The line below assumes square matrix:\n",
    "    mask = torch.ones((queries_keys.shape[-1], queries_keys.shape[-1]), dtype=torch.bool).triu(diagonal=1)\n",
    "\n",
    "    attn_scores = queries_keys.masked_fill(mask, float(\"-inf\"))   # This is for all attn heads so propagation must be going on\n",
    "\n",
    "    attn_weights = F.softmax(attn_scores, dim = -1) # This is for all attn heads to propagation must be going on\n",
    "\n",
    "    head_context_vectors = attn_weights@values # This is for all ...  so it's head_context_vectorS !!! (multiple)\n",
    " \n",
    "    # Now we use einsum to rearrange the heads away\n",
    "\n",
    "    multihead_context_vector = rearrange(head_context_vector, 'b h s d -> b s (h d)')\n",
    "\n",
    "    # This is where you add the Memory if this is the last multi-head attn layer -> DO THIS BEFORE RETURNING Wo OUTPUT !!!\n",
    "\n",
    "    multihead_context_vector = self.Wo(multihead_context_vector)\n",
    "\n",
    "    return multihead_context_vector    \n",
    "\n",
    "# 41:50\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

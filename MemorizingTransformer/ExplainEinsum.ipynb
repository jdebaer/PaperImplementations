{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e922b0fb-dc74-4425-9080-d5e60fd56797",
   "metadata": {},
   "source": [
    "Memory expects incoming queries to be (b_s, seq_len, embed_dim)     (note that queries and in fact keys, check this)\n",
    "\n",
    "Memory returns results in (b_s, seq_len, top_k, 2, embed_dim)\n",
    "\n",
    "In the video, the embed_dim is cut into pieces and each attn head gets a piece, so what needs to happen with the results is this:\n",
    "\n",
    "1. Split out keys and values:\n",
    "    keys: (b_s, seq_len, top_k, embed_dim)\n",
    "    values: (b_s, seq_len, top_k, embed_dim)\n",
    "2. Cut both in pieces to prep feeding to the attn heads:\n",
    "    keys: (b_s, seq_len, top_k, heads, head_dim)\n",
    "    values: (b_s, seq_len, top_k, heads, head_dim)\n",
    "3. Now we need to pull the head dimension forward as we're going to feed it all into each head (except the batch):\n",
    "    queries: (b_s, heads, seq_len, head_dim)\n",
    "    keys: (b_s, heads, seq_len, head_dim, top_k) (we also need to pull head_dim forward)\n",
    "    values: (b_s, heads, seq_len, top_k, head_dim)\n",
    "    # This means we're also chopping up the memories and feeding the pieces to the attn heads\n",
    "\n",
    "4. Then we also want to do self attention in each head with the pieces, where\n",
    "    qk is obtained by (b_s, heads, seq_len, head_dim) @ (b_s, heads, seq_len, head_dim, top_k) -> (b_s, heads, seq_len, top_k)\n",
    "\n",
    "[1, 2, 3]  @  [2,4] --> [2 + 8 + 18, 4 + 10 + 21] -> basically you get an attention score for each top_k, for each token in the sequence     \n",
    "              [4,5]\n",
    "              [6,7]\n",
    "\n",
    "    then we must also multiply that with the value:\n",
    "\n",
    "    (b_s, heads, seq_len, top_k) @ (b_s, heads, seq_len, top_k, head_dim) -> \n",
    "\n",
    "   [2, 3] @ [1, 2, 3, 4, ...]  --> [2 + 15, 4 + 18, ... ] , we essentially add the pieces up \n",
    "            [5, 6, 7, 8, ...] \n",
    "\n",
    "                                                                                   \n",
    "In our implemtation however, we feed each attention head the complete embed_dim so we don't need to do this, we can reuse the existing logic.\n",
    "\n",
    "However, below is how it's done in the videa with einsum (without top_k).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de2bf13e-6447-447b-bd55-091dfcbd9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "b_s = 8\n",
    "seq_len = 512\n",
    "num_attn_heads = 4\n",
    "head_dim = 4\n",
    "\n",
    "queries = torch.randn(b_s, seq_len, num_attn_heads * head_dim)\n",
    "keys = torch.randn(b_s, seq_len, num_attn_heads * head_dim)\n",
    "\n",
    "queries = rearrange(queries, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=num_attn_heads)\n",
    "keys = rearrange(keys, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=num_attn_heads)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912bf81a-c419-435e-9c6a-0dd4fb3cf68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "\n",
    "#nn.Softmax is an nn.Module, which can be initialized e.g. in the __init__ method of your model and used in the forward. \n",
    "#torch.softmax() and nn.functional.softmax are equal and I would recommend to stick to nn.functional.softmax, since itâ€™s documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f723bde-fd59-4ba8-ac78-a1f17e5a8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 and 2 until you see Stage 3\n",
    "\n",
    "# Every decoder needs these:\n",
    "\n",
    "seq_len = 512\n",
    "embed_dim = 256\n",
    "#head_dim = 32 -> in our implementation we calculate the equivalent\n",
    "num_attn_heads = 2\n",
    "# See diffence in ouput shape: last dimension is seq_len/num_atten_heads\n",
    "batch_size = 16\n",
    "#scaling_factor = head_dim ** -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b36936-23fc-4115-9e43-fada4696cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific to our implementation\n",
    "\n",
    "attn_head_input_dim = embed_dim\n",
    "attn_head_output_dim = embed_dim // num_attn_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309efdf8-6571-4b4a-8609-3868d06ff104",
   "metadata": {},
   "source": [
    "About bias: https://www.turing.com/kb/necessity-of-bias-in-neural-networks#what-is-bias-in-a-neural-network?\n",
    "\n",
    "However, for certain types of layers, such as transformers and convolutional layers, including a bias term is unnecessary and adds unnecessary overhead to the model. The reason for this is that these layers are typically followed by a normalization layer, such as Batch \n",
    "Normalization or Layer Normalization. These normalization layers center the data at mean=0 (and std=1), effectively removing any bias.\n",
    "Therefore, it is common practice to omit the bias term in transformers and convolutional layers that are preceded by a normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ed949b0-545e-4c95-a0d2-f4d430d3bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake input data in a tensor just to push it through here - token ids are mapped to embeddings.\n",
    "# Stage 1: without batch dimension\n",
    "#input_data = torch.randn(seq_len, embed_dim)\n",
    "# Stage 2: simply add the batch dimension\n",
    "input_data = torch.randn(batch_size, seq_len, embed_dim)\n",
    "#Look at shape below to see the difference in the output - all the code remains the same!\n",
    "\n",
    "# Stage 3: we putting all of the below in a nn.module that return the head's context vector in its forward()\n",
    "\n",
    "class AttnHead(nn.Module):\n",
    "\n",
    "    def __init__(self, attn_head_input_dim, attn_head_output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Just add self..\n",
    "        self.Wq = nn.Linear(attn_head_input_dim, attn_head_output_dim, bias=False) \n",
    "        self.Wk = nn.Linear(attn_head_input_dim, attn_head_output_dim, bias=False)\n",
    "        self.Wv = nn.Linear(attn_head_input_dim, attn_head_output_dim, bias=False) \n",
    "\n",
    "    def forward(self, input_data):\n",
    "\n",
    "        # Just add self..\n",
    "        queries = self.Wq(input_data)     \n",
    "        keys = self.Wk(input_data)\n",
    "        values = self.Wv(input_data)\n",
    "\n",
    "        dim_of_key = keys.size(-1) \n",
    "\n",
    "        attn_scores = queries @ keys.transpose(-2,-1)/sqrt(dim_of_key) \n",
    "\n",
    "        mask = torch.ones((attn_scores.shape[-1], attn_scores.shape[-1]), dtype=torch.bool).triu(diagonal=1)\n",
    "\n",
    "        attn_scores = attn_scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim = -1)\n",
    "\n",
    "        head_context_vector = attn_weights@values \n",
    "\n",
    "        return head_context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b3a1a-888f-4dfc-bb60-c2fcb1fb3ee4",
   "metadata": {},
   "source": [
    "Each head is producing a head_context_vector - these are then brought back to embed_size via Wo\n",
    "We do that in Stage 3, separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f17854c-563b-4da1-b136-d29d808f65b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3 == bring it back to embed_size via Wo\n",
    "\n",
    "# Wo is defined in the multi-head level, while Wq/Wk/Wv are defined at the level of the attention head\n",
    "\n",
    "Wo = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "attn_heads = nn.ModuleList(\n",
    "    [AttnHead(attn_head_input_dim, attn_head_output_dim) for _ in range(num_attn_heads)]\n",
    ")\n",
    "\n",
    "concatenated_head_context_vectors = torch.cat(\n",
    "               [attn_head(input_data) for attn_head in attn_heads], dim=-1\n",
    ")\n",
    "\n",
    "multihead_context_vector = Wo(concatenated_head_context_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

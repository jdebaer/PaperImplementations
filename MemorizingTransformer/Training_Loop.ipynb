{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92445e16-b7a8-4441-9b79-705513cc57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c358a-1653-4e5b-88b0-6c10b4018e05",
   "metadata": {},
   "source": [
    "Note: memorizing transformer is trained with long documents, where the document is longer than the seq_len (input size), so that we have to break the document into chunks, so that for each chunk we look backwards and see if we can find info in XL recurrent mem or in kNN stored memory to help us predict -> arxiv-summarization is a good HF dataset as it contains long doucments  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a82e3cd-27ff-4985-86e5-4fddfcf51170",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"ccdv/arxiv-summarization\", split=\"train\", streaming=True)\n",
    "raw_dataset = list(dataset.take(10)) # take a subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb442a62-f394-4e33-a26b-94cdc5a56f75",
   "metadata": {},
   "source": [
    "Note: normally we would shuffle the dataset to avoid bias, but here we consciously don't do that, because we want our transformer to use memory about the past sequences -> we are going to feed in the same article, chunk by chunk / sequence by sequence. This is mentioned in the paper. We're calling these chunks/sequences \"segments\" from now on.\n",
    "\n",
    "Normally (vanilla) a batch contains a certain number of sequences of seq_len/input_len tokens, and the whole sequence is used to train in parallel and there is not relation to other sequences (ideally it's all shuffled and mentioned above).\n",
    "\n",
    "Here however we need that relationship between a sequence and the past sequence, so what we do is in batch 1 we put all the first sequences/segments of b_s docs, then batch 2 will have all the 2nd segments of these same docs, and so on. So each \"row\" that spans the batches are segments that come from the same document. This implies that each \"row\" has its own training \"context\" where the memory will be used/trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ba176e-952d-4727-94be-699d974620a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments = 10    # This means we take 10 segments of each doc. This is not a lot but this way we don't have to pad.\n",
    "seg_len = 512   # seq_len usually but here we'll go with seg_len - same thing\n",
    "chunk_size = segments * seg_len # Not sure why chunk is needed at this point TBD\n",
    "chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eccc3be-7c34-4c29-a331-c63c54adfb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token set length:  70\n",
      "toket set:  \n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/rbg9dqnx4h534h57wgqgvpkr0000gn/T/ipykernel_42967/3443583100.py:25: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  raw_articles_as_token_ids = [np.fromstring(raw_article, dtype=np.uint8) for raw_article in raw_articles]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([46, 5120])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_articles = [x['article'] for x in raw_dataset] # The other key is 'abstract' -> see with raw_dataset[0].keys\n",
    "# 5120 is min size so any articles shorter can be discarded\n",
    "raw_articles = [x for x in raw_articles if len(x) > 5120] # Not sure why > and not >=\n",
    "# Note that we're doing character-based tokenization here to keep the math simple: normally a token will have multiple chars but not now\n",
    "\n",
    "unique_tokens = set(''.join([raw_article for raw_article in raw_articles]))\n",
    "\n",
    "print(\"token set length: \", len(unique_tokens))\n",
    "print(\"toket set: \", ''.join(sorted(unique_tokens)))\n",
    "\n",
    "# Again: taking a massive shortcut here by using characters as tokens -> replace by BPE or subword\n",
    "# We now need to convert each token in our set to a token_id (number) - quick way to do this is to just use the unicode int for it\n",
    "# uint8 is the first 128 and that will cover the 70 that we have - there will be \"dead\" slots (numbers) for which we will create a\n",
    "# \"dead\" embedding vector, but ok for now, we'll switch to BPE or Subword anyway.\n",
    "\n",
    "# quick play with this:\n",
    "#print(raw_articles[0][:20])\n",
    "#token_ids = np.fromstring(raw_articles[0], dtype=np.uint8)[:20]\n",
    "#token_ids\n",
    "#tokens = ''.join([chr(token_id) for token_id in token_ids])\n",
    "#tokens\n",
    "\n",
    "# Token_id all the articles - note that string is converted to array here:\n",
    "\n",
    "raw_articles_as_token_ids = [np.fromstring(raw_article, dtype=np.uint8) for raw_article in raw_articles]\n",
    "\n",
    "# Now make sure that each article is clipped at a certain number of chunk sizes\n",
    "\n",
    "def clip_article(article, chunk_size):\n",
    "    remainder = len(article) % chunk_size\n",
    "    return article[:-remainder]\n",
    "\n",
    "clipped_raw_articles_as_token_ids = [clip_article(raw_article, chunk_size) for raw_article in raw_articles_as_token_ids]\n",
    "\n",
    "# What we have at this point is a list of articles, where each article is in token_ids and a multiple of chunk_size and then cut off\n",
    "\n",
    "\n",
    "#for article in clipped_raw_articles_as_token_ids:\n",
    "#    print(article.reshape(-1,chunk_size).shape)\n",
    "\n",
    "\n",
    "clipped_raw_arts_as_t_ids_as_chunks = np.array([article.reshape(-1, chunk_size) for article in clipped_raw_articles_as_token_ids], dtype=object)\n",
    "\n",
    "# The reshape transforms each article from an array of token ids to a 2-D array (#chunks, chunk_size) \n",
    "# #chunks can vary dep. on article length\n",
    "\n",
    "#clipped_raw_arts_as_t_ids_as_chunks.shape\n",
    "# (10,) second dim is not given as it's a ragged array and the dim size varies dep. on length of article\n",
    "\n",
    "processed_data = torch.tensor(np.concatenate(clipped_raw_arts_as_t_ids_as_chunks), dtype=torch.long)\n",
    "processed_data.shape\n",
    "\n",
    "# numbers for 10 articles:\n",
    "#(5, 5120)\n",
    "#(3, 5120)\n",
    "#(5, 5120)\n",
    "#(4, 5120)\n",
    "#(10, 5120)\n",
    "#(7, 5120)\n",
    "#(2, 5120)\n",
    "#(3, 5120)\n",
    "#(2, 5120)\n",
    "#(5, 5120)\n",
    "#\n",
    "# --> torch.Size([46, 5120]) so concat happens on first dimension\n",
    "# We now have chunks of chunk_size to feed in for training where chunks relate to each other (from same article) until article changes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c147e3-9ac7-4f08-8dca-73c9c2df757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = iter(DataLoader(processed_data, batch_size=8, shuffle=True))\n",
    "\n",
    "# We're going to have a batch of 8 random chunks. The chunks will not be related. However we're then going to push these chunks through\n",
    "# the model in parallel, and we'll split each chunk up in seg_len parts that WILL be sequentially related."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6fd052-d41e-40f2-ad19-8a8769ac7619",
   "metadata": {},
   "source": [
    "THIS IS JUST FOR UNDERSTANDING\n",
    "\n",
    "example = next(loader) # Get a batch\n",
    "print(example.shape)\n",
    "# torch.Size([8, 5120])\n",
    "seqs, labels = example[:,:-1], example[:,1:]\n",
    "print(seqs[0][:10])\n",
    "print(labels[0][:10])\n",
    "#tensor([116, 104, 101,  32, 100, 105, 114,  97,  99,  32]) -> label is the token to the right\n",
    "#tensor([104, 101,  32, 100, 105, 114,  97,  99,  32, 115]) -> the above but shifted left\n",
    "\n",
    "# Now we want to feed in b_s parts of seg_len, with each segment coming from a different chunk, \n",
    "# but then the next segment coming from the same chuck again \n",
    "\n",
    "seqs.chunk(10, dim=-1)[0].shape\n",
    "# We split each chunk verticall in 10 pieces of 512, and then get a tensor of size b_s with the first 512 token_ids from each chunk\n",
    "# torch.Size([8, 512])  so we went from 5120 \n",
    "# Note that for segs.chunk(10, dim=-1)[0].shape we get\n",
    "# torch.Size([8, 511]) \n",
    "# because there are not 5120 but only 5119 token_ids in segs and labels (because we either lose the last or first token)\n",
    "\n",
    "def decode_text(token_ids):\n",
    "    return ''.join([chr(token_id) for token_id in token_ids])\n",
    "\n",
    "for seqs_segment, labels_segment in zip(seqs.chunk(10, dim=-1), labels.chunk(10, dim=-1)):\n",
    "    print(decode_text(seqs_segment[0]), \"\\n ***************** \\n\")\n",
    "\n",
    "# In the above we chunk each chunk in the batch up vertically in 10 parts and we loop 10 times, where at each iteration\n",
    "# we have a 8 (b_s) slices. The slices don't relate to each other (they are from different chunks) but each slice relates to the\n",
    "# slice in the same \"position\" in the previous batch.\n",
    "\n",
    "# So in print(decode_text(seqs_segment[0]) you can go up to 7 for a b_s of 8, but 8 throws an error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41452158-a70a-4923-bf0b-cff9990483ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.795375871658325\n",
      "4.3197229385375975\n",
      "3.697280502319336\n"
     ]
    }
   ],
   "source": [
    "# Dummy model\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(128, 16), # (vocab_size, embed_dim)\n",
    "    nn.Linear(16, 150),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(150,150),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(150, 128) # back to vocab_size\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "model.train()\n",
    "\n",
    "# We have 46 chunks, so with a batch size of 8 we can call next 6 times -> 5 * 8 == 40, call 6 will only have 6 and not 8 in the batch\n",
    "\n",
    "segments = 10 # Just because 5120 / 10 == 512 \n",
    "\n",
    "loader = iter(DataLoader(processed_data, batch_size=8, shuffle=True))\n",
    "\n",
    "for idx in range(6):\n",
    "    \n",
    "    batch = next(loader)  # (b_s, seq_len) to (8, 5120) here\n",
    "    seqs, labels = batch[:,:-1], batch[:,1:]\n",
    "    train_loss = 0\n",
    "\n",
    "    # This is special, normally we would just loop through the batch, but here we need to cut up each chunk\n",
    "\n",
    "    # The use of chunk() is confusing here, since we're using it to cut up what we otherwise call a chunk, which is the\n",
    "    # article that is clipped to a size of 5120 == segments * seqment length or 10 * 512\n",
    "\n",
    "    for seqs_segment, labels_segment in zip(seqs.chunk(segments, dim=-1), labels.chunk(segments, dim=-1)): \n",
    "        # \"segments\" passes of (b_s, chunk_size/segments) or (8, 512 here)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seqs_segment) # The model is trained on the batch-segment in parallel on the GPU\n",
    "        y_pred = y_pred.transpose(2,1) # Just for the loss function\n",
    "\n",
    "        loss = loss_fn(y_pred, labels_segment) # loss for all batch-segments\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # We just keep adding to train_loss, segment per segment, and in the end we have it for the whole batch\n",
    "\n",
    "    if idx % 2 == 0: # Every 5 batches (chunks) print the training loss for that batch\n",
    "        print(train_loss / segments) # We average over the segments\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

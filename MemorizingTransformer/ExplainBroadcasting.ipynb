{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11eae51-0fff-4d1d-9344-4aed99f7d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mul()\n",
    "# Elementwise multiplication\n",
    "# Broadcasts\n",
    "\n",
    "torch.dot()\n",
    "# Dot product of 2 one-dimentional only (!) tensors\n",
    "# Does no broadcasting\n",
    "\n",
    "torch.mm()\n",
    "# Do matrix mul of two 2D matrices\n",
    "# Also does not broadcast\n",
    "\n",
    "torch.bmm()\n",
    "# Do matrix mul of two 3D matrices: (b x n x m) @ (b x m x p) -> (b x n x p)\n",
    "# Also does not broadcast\n",
    "\n",
    "torch.matmul() == @\n",
    "# Depending on input will do dot(), mm() or bmm() broadcasted to tensors of any size\n",
    "# This does broadcasting so may lead to undesired results, be careful with it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba507d0-69c6-46ab-a055-7225bed48d2e",
   "metadata": {},
   "source": [
    "Broadcasting:\n",
    "\n",
    "Note - depends on operation first! If the operation itself is not broadcasting, then no broadcasting will happend \n",
    "despite broadcastable.\n",
    "\n",
    "Two tensors are broadcastable if (and):\n",
    "\n",
    "1. Each tensor has at least one dim\n",
    "2. When iterating of the the dims, starting at the last one (or): \n",
    "    The dim sizes must be equal OR\n",
    "    One of the dim sizes is 1\n",
    "    One of the dims does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9695a6f1-1a30-48d1-b76e-30cb4ed833d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64, 1152, 1, 16])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(1, 64, 1152, 1, 8)\n",
    "b = torch.randn(10, 1, 1152, 8, 16)\n",
    "\n",
    "c = a @ b\n",
    "c.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28087c3-978e-4374-9c27-1c141a4af18d",
   "metadata": {},
   "source": [
    "What we see is that the first dim in a is copied out 10 times, and same for second dim in b\n",
    "The 4th dim in c is not a result of broadcasting, there it's just doing (1x8) @ (8x16) -> (1,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c685ea-bc9a-4816-93cd-1289ed58429e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 1152, 1, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 64, 1152, 1, 8)\n",
    "# You start at the end, so 16 is matched with 8 for the rules (last dimensions first)\n",
    "b = torch.randn(8, 16)\n",
    "\n",
    "c = a@b\n",
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2a5007a-217d-40f0-974a-b104a3228a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [1., 2., 3., 4.],\n",
       "        [1., 2., 3., 4.],\n",
       "        [1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here an example of what we would need to do without broadcasting:\n",
    "\n",
    "a = torch.arange(4)\n",
    "# tensor([0, 1, 2, 3])\n",
    "\n",
    "b = torch.ones(4,4)\n",
    "#tensor([[1., 1., 1., 1.],\n",
    "#        [1., 1., 1., 1.],\n",
    "#        [1., 1., 1., 1.],\n",
    "#        [1., 1., 1., 1.]])\n",
    "\n",
    "# Without broacasting:\n",
    "\n",
    "a = a.expand(4,4)\n",
    "#tensor([[0, 1, 2, 3],\n",
    "#        [0, 1, 2, 3],\n",
    "#        [0, 1, 2, 3],\n",
    "#        [0, 1, 2, 3]])\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bcd6ddf-4692-403d-8151-da38c3e8feda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [1., 2., 3., 4.],\n",
       "        [1., 2., 3., 4.],\n",
       "        [1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#With broadcasting:\n",
    "\n",
    "a + 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f203ae6-f5cc-40b5-b953-f8baf927e56f",
   "metadata": {},
   "source": [
    "Very common case where broadcasting may not tell you you're making an error is in loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82e905f1-b76a-4929-a966-d67123acf78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jdebaer/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([3, 5])) that is different to the input size (torch.Size([2, 3, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.7476)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "input = torch.randn(2,3,5)\n",
    "label = torch.rand(3,5)\n",
    "\n",
    "loss = loss_function(input, label)\n",
    "loss\n",
    "\n",
    "# This mistake was happening so much that the warning below was added in the loss fuction code :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4093dd-b6ea-4bd3-a1ed-d067a4d29103",
   "metadata": {},
   "source": [
    "Multidim matrix multiplication\n",
    "------------------------------\n",
    "It's always only the last two dims that are matrix multiplied, with our without broadcasting as per all of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2ecc274-6358-42d5-98a9-c3bfcbf5b16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[36., 36.],\n",
       "         [36., 36.]],\n",
       "\n",
       "        [[56., 56.],\n",
       "         [56., 56.]],\n",
       "\n",
       "        [[80., 80.],\n",
       "         [80., 80.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create some 3D matrices\n",
    "\n",
    "a = torch.ones(3,2,2)\n",
    "a[0] = 3\n",
    "a[1] = 4\n",
    "a[2] = 5\n",
    "\n",
    "#tensor([[[3., 3.],\n",
    "#         [3., 3.]],\n",
    "#        [[4., 4.],\n",
    "#         [4., 4.]],\n",
    "#        [[5., 5.],\n",
    "#         [5., 5.]]])\n",
    "\n",
    "b = torch.ones(3,2,2)\n",
    "b[0] = 6\n",
    "b[1] = 7\n",
    "b[2] = 8\n",
    "\n",
    "#tensor([[[6., 6.],\n",
    "#         [6., 6.]],\n",
    "#        [[7., 7.],\n",
    "#         [7., 7.]],\n",
    "#        [[8., 8.],\n",
    "#         [8., 8.]]])\n",
    "\n",
    "#a@b  # We know it's going to be (3,2,2), with matmul on the last two dims\n",
    "\n",
    "a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e92add2-acc2-4427-8f9d-ad05bd530dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.ones(2,2)\n",
    "test[0][0] = 0\n",
    "test[0][1] = 1\n",
    "test[1][0] = 2\n",
    "test[1][1] = 3\n",
    "#test\n",
    "# Printing it out does so in rxc format\n",
    "#tensor([[0., 1.],\n",
    "#        [2., 3.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16c91477-8ce9-424d-99d8-71cf9d15c028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 42.,  42.],\n",
       "          [ 42.,  42.]],\n",
       "\n",
       "         [[ 42.,  42.],\n",
       "          [ 42.,  42.]],\n",
       "\n",
       "         [[ 42.,  42.],\n",
       "          [ 42.,  42.]]],\n",
       "\n",
       "\n",
       "        [[[ 64.,  64.],\n",
       "          [ 64.,  64.]],\n",
       "\n",
       "         [[ 64.,  64.],\n",
       "          [ 64.,  64.]],\n",
       "\n",
       "         [[ 64.,  64.],\n",
       "          [ 64.,  64.]]],\n",
       "\n",
       "\n",
       "        [[[ 90.,  90.],\n",
       "          [ 90.,  90.]],\n",
       "\n",
       "         [[ 90.,  90.],\n",
       "          [ 90.,  90.]],\n",
       "\n",
       "         [[ 90.,  90.],\n",
       "          [ 90.,  90.]]],\n",
       "\n",
       "\n",
       "        [[[120., 120.],\n",
       "          [120., 120.]],\n",
       "\n",
       "         [[120., 120.],\n",
       "          [120., 120.]],\n",
       "\n",
       "         [[120., 120.],\n",
       "          [120., 120.]]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's do 4D!\n",
    "\n",
    "a = torch.ones(4,3,2,2) #-> We're going to see 3 2x2 matrices each of them with 3s in them, the 3 with 4s in them, etc.\n",
    "a[0] = 3\n",
    "a[1] = 4\n",
    "a[2] = 5\n",
    "a[3] = 6\n",
    "\n",
    "# Read this as \"we have 4 groups of 3 groups of 2x2 matrices\"\n",
    "\n",
    "#tensor([[[[3., 3.],\n",
    "#          [3., 3.]],\n",
    "#         [[3., 3.],\n",
    "#          [3., 3.]],\n",
    "#         [[3., 3.],\n",
    "#          [3., 3.]]],\n",
    "#        [[[4., 4.],\n",
    "#          [4., 4.]],\n",
    "#         [[4., 4.],\n",
    "#          [4., 4.]],\n",
    "#         [[4., 4.],\n",
    "#          [4., 4.]]],\n",
    "#        [[[5., 5.],\n",
    "#          [5., 5.]],\n",
    "#         [[5., 5.],\n",
    "#          [5., 5.]],\n",
    "#         [[5., 5.],\n",
    "#          [5., 5.]]],\n",
    "#        [[[6., 6.],\n",
    "#          [6., 6.]],\n",
    "#         [[6., 6.],\n",
    "#          [6., 6.]],\n",
    "#         [[6., 6.],\n",
    "#          [6., 6.]]]])\n",
    "\n",
    "b = torch.ones(4,3,2,2)\n",
    "b[0] = 7\n",
    "b[1] = 8\n",
    "b[2] = 9\n",
    "b[3] = 10\n",
    "\n",
    "a@b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c1c67-57b4-48c4-8241-309e5afa855d",
   "metadata": {},
   "source": [
    "Difference between view and reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7817f115-adc2-4406-8a6e-4d024195d435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23]])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23]])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(24)\n",
    "print(a.view(4,6))\n",
    "print(a)\n",
    "print(a.reshape(4,6))\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0641c-4101-4392-89f1-5bb7b3bd6b5c",
   "metadata": {},
   "source": [
    "They seems to be doing the same, however view only works on contiguous data, so it won't work after you have transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b60431e3-861f-4576-b54d-5c9fae2e4907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23]])\n",
      "tensor([[ 0,  6, 12, 18],\n",
      "        [ 1,  7, 13, 19],\n",
      "        [ 2,  8, 14, 20],\n",
      "        [ 3,  9, 15, 21],\n",
      "        [ 4, 10, 16, 22],\n",
      "        [ 5, 11, 17, 23]])\n",
      "4570799040\n",
      "4570799040\n"
     ]
    }
   ],
   "source": [
    "a = a.view(4,6) # Can also use reshape, same result\n",
    "a_T = a.transpose(0,1)\n",
    "print(a)\n",
    "print(a_T)\n",
    "print(a.data_ptr()) # Returns address of first element in tensor\n",
    "print(a_T.data_ptr()) # It's still in the same starting location, but has been rearranged and no longer contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e4dd793-1648-4ab6-adbb-38e2ad1d390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([[ 0,  6, 12, 18,  1,  7],\n",
      "        [13, 19,  2,  8, 14, 20],\n",
      "        [ 3,  9, 15, 21,  4, 10],\n",
      "        [16, 22,  5, 11, 17, 23]])\n",
      "4319112960\n"
     ]
    }
   ],
   "source": [
    "#a_T.view(4,6) # Error, because:\n",
    "\n",
    "print(a_T.is_contiguous())\n",
    "\n",
    "#However: (at this point a_T is (6,4) because it's transposed.\n",
    "a_T_reshaped = a_T.reshape(4,6) # Reshape will make a new copy of the non-contiguous tensor, \n",
    "                                # but only if it really has to reshape, so NOT is we use (6,4) here.\n",
    "print(a_T_reshaped)\n",
    "print(a_T_reshaped.data_ptr())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

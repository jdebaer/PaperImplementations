{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "912bf81a-c419-435e-9c6a-0dd4fb3cf68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "\n",
    "#nn.Softmax is an nn.Module, which can be initialized e.g. in the __init__ method of your model and used in the forward. \n",
    "#torch.softmax() and nn.functional.softmax are equal and I would recommend to stick to nn.functional.softmax, since itâ€™s documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f723bde-fd59-4ba8-ac78-a1f17e5a8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 and 2 until you see Stage 3\n",
    "\n",
    "# Every decoder needs these:\n",
    "\n",
    "seq_len = 512\n",
    "embed_dim = 256\n",
    "#head_dim = 32 -> in our implementation we calculate the equivalent\n",
    "num_attn_heads = 2\n",
    "# See diffence in ouput shape: last dimension is seq_len/num_atten_heads\n",
    "batch_size = 16\n",
    "#scaling_factor = head_dim ** -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b36936-23fc-4115-9e43-fada4696cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific to our implementation\n",
    "\n",
    "attn_head_input_dim = embed_dim\n",
    "attn_head_output_dim = embed_dim // num_attn_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309efdf8-6571-4b4a-8609-3868d06ff104",
   "metadata": {},
   "source": [
    "About bias: https://www.turing.com/kb/necessity-of-bias-in-neural-networks#what-is-bias-in-a-neural-network?\n",
    "\n",
    "However, for certain types of layers, such as transformers and convolutional layers, including a bias term is unnecessary and adds unnecessary overhead to the model. The reason for this is that these layers are typically followed by a normalization layer, such as Batch \n",
    "Normalization or Layer Normalization. These normalization layers center the data at mean=0 (and std=1), effectively removing any bias.\n",
    "Therefore, it is common practice to omit the bias term in transformers and convolutional layers that are preceded by a normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ed949b0-545e-4c95-a0d2-f4d430d3bcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fake input data in a tensor just to push it through here - token ids are mapped to embeddings.\n",
    "# Stage 1: without batch dimension\n",
    "#input_data = torch.randn(seq_len, embed_dim)\n",
    "# Stage 2: simply add the batch dimension\n",
    "input_data = torch.randn(batch_size, seq_len, embed_dim)\n",
    "#Look at shape below to see the difference in the output - all the code remains the same!\n",
    "\n",
    "# Note that we don't need the batch dimension for these - each head has its own set of Wq/Wk/Wv and they are trained across batches\n",
    "# In the batched version (stage 2) these W's only operate on the last two dimensions for each batch element\n",
    "Wq = nn.Linear(attn_head_input_dim, attn_head_output_dim, bias=False) # After training, Wq has information about tokens\n",
    "Wk = nn.Linear(attn_head_input_dim, attn_head_output_dim, bias=False) # After training, Wk has information about surrounding tokens\n",
    "Wv = nn.Linear(attn_head_input_dim, attn_head_output_dim, bias=False) \n",
    "\n",
    "queries = Wq(input_data)     # (seq_len x embed_size) @ (embed_size, output_dim) -> (seq_len, outpu_dim) w/ latter the dim ot context vector\n",
    "keys = Wk(input_data)\n",
    "values = Wv(input_data)\n",
    "\n",
    "dim_of_key = keys.size(-1) \n",
    "\n",
    "attn_scores = queries @ keys.transpose(-2,-1)/sqrt(dim_of_key) # divide by \"inner\" dimension == key_dim == query_dim (must)\n",
    "# attn_scores.shape -> (seq_len, seq_len) with dot product of the embeddings in each cell\n",
    "# attn_scores are the normalized dot products of the (reduced) embedding vectors for the token_ids (as queries and keys)\n",
    "\n",
    "# Here is where we will apply masking (before we apply the softmax)\n",
    "\n",
    "# attn_scores is always going to be square (every token has to be resonated with every token)\n",
    "mask = torch.ones((attn_scores.shape[-1], attn_scores.shape[-1]), dtype=torch.bool).triu(diagonal=1)\n",
    "\n",
    "# tensor([[False,  True,  True,  ...,  True,  True,  True], ...\n",
    "\n",
    "attn_scores = attn_scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "attn_weights = F.softmax(attn_scores, dim = -1)\n",
    "# tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00], ...\n",
    "\n",
    "# Now we need to apply these weights to the value vectors -> these value vectors contain the information that's passed on \n",
    "# to the next layer.\n",
    "# The queries and keys go thought a softmax, so lots of the information gets lost there, but that's why we're only using them to weigh.\n",
    "\n",
    "\n",
    "head_context_vector = attn_weights@values # (seq_len, seq_len) @ (seq_len x output_dim) -> (seq_len, output_dim)\n",
    "#head_context_vector2 = torch.bmm(attn_weights, values) -> same result\n",
    "\n",
    "head_context_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b3a1a-888f-4dfc-bb60-c2fcb1fb3ee4",
   "metadata": {},
   "source": [
    "Each head is producing a head_context_vector - these are then brought back to embed_size via Wo\n",
    "We do that in Stage 3, separate file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

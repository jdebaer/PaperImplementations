{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a580795-be56-4b9e-97bc-1f942939d1e9",
   "metadata": {},
   "source": [
    "We will do relative positional encoding with modifications from the T5 paper\n",
    "\n",
    "The basic idea of relative pos encoding is using a matrix like this:\n",
    "\n",
    "tensor([[ 0, 1, 2, 3],\n",
    "        [-1, 0, 1, 2],\n",
    "        [-2,-1, 0, 1],\n",
    "        [-3,-2,-1, 0]])\n",
    "\n",
    "So that each positioning is encoded relative to the \"current token\" (query).\n",
    "\n",
    "Note that for decoder self-attention we want to use causal masking (meaning: we don't want to give ANY info about future tokens)\n",
    "so it will look like this:\n",
    "\n",
    "tensor([[ 0, 0, 0, 0],\n",
    "        [-1, 0, 0, 0],\n",
    "        [-2,-1, 0, 0],\n",
    "        [-3,-2,-1, 0]])\n",
    "\n",
    "and in this implementation we will not use negatives (does not matter) so it will be\n",
    "\n",
    "tensor([[ 0, 0, 0, 0],\n",
    "        [ 1, 0, 0, 0],\n",
    "        [ 2, 1, 0, 0],\n",
    "        [ 3, 2, 1, 0]])\n",
    "\n",
    "We'll then apply \"buckets\" where ranges will get the same pos. encoding, with the first 3 \"past\" tokens have their exact position, so it will look something like this for the last row (larger seq_len):\n",
    "\n",
    "[7,7,7,7,6,6,6,5,5,5,4,4,3,2,1,0,]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d804abc-9c1f-482b-87d2-468421999e7e",
   "metadata": {},
   "source": [
    "Some general info about nn.Embedding:\n",
    "\n",
    "Both nn.Linear and nn.Embedding will given you, in your example, a 3-dim vector. Thatâ€™s the whole point, i.e., to convert a token \n",
    "into an ideally meaningful vectors (i.e., a numeric and fix-sized representation of a word). The difference is w.r.t. the input\n",
    "nn.Linear expects a one-hot vector of the size of the vocabulary with the single 1 at the index representing the specific word.\n",
    "nn.Embedding just expects this index (and not a whole vector).\n",
    "\n",
    "However, if both nn.Linear and nn.Embedding would be initialized with the same weights, their outputs would be exactly the same.\n",
    "\n",
    "Yes, by default, the weights of both layers will be modified during the training process. In this respect, there are like any other \n",
    "layers in your network. However, you can tell the network not to modify the weights of any specific layer; I think it would look \n",
    "something like this:\n",
    "\n",
    "embedding = nn.Embedding(10, 3)\n",
    "embedding = weight.requires_grad = False\n",
    "\n",
    "This makes sense if you use pretrained word embeddings such as Word2Vec or Glove. If you initialize your weights randomly, you \n",
    "certainly want them to be modified during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a886e0f-c91d-4357-b714-938c55504a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0],\n",
       "        [5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0],\n",
       "        [5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0],\n",
       "        [5, 5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0],\n",
       "        [5, 5, 5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "num_buckets = 6\n",
    "max_distance = 20    # Max sequence length - this will be 128 as per the paper\n",
    "seq_len = 15         # This is query length\n",
    "max_context_len = 15 # This is key length - normally same as query length but not for XL Trfrmrs where we concat keys as part of recurrency\n",
    "\n",
    "# Now we construct a matrix as per the above\n",
    "\n",
    "q_pos = torch.arange(seq_len, dtype=torch.long)               # Top row\n",
    "k_pos = torch.arange(max_context_len, dtype=torch.long) \n",
    "\n",
    "# Trick:\n",
    "#[0, 1, 2, 3] - [[0], == (via broadcasting) [[0, 1, 2, 3] - [[0, 0, 0, 0], == [[ 0, 1, 2, 3], \n",
    "#                [1],                        [0, 1, 2, 3]    [1, 1, 1, 1],     [-1, 0, 1, 2],\n",
    "#                [2],                        [0, 1, 2, 3]    [2, 2, 2, 2],     [-2,-1, 0, 1],\n",
    "#                [3]]                        [0, 1, 2, 3]]   [3, 3, 3, 3]]     [-3,-2,-1, 0]]\n",
    "\n",
    "# So we need to convert q_pos to a column vector:\n",
    "q_pos = q_pos.reshape(q_pos.shape[0], 1)\n",
    "\n",
    "rel_pos = k_pos - q_pos\n",
    "#rel_pos # With seq_len 10 for query and max_context_len 15 for (concatenated) keys this gives:\n",
    "# Query goes \"up/down\" since we only have the current sequence, but we match it with a concat of keys for recurrence ->>\n",
    "#tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14],\n",
    "#        [-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
    "#        [-2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
    "#        [-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
    "#        [-4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
    "#        [-5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
    "#        [-6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
    "#        [-7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7],\n",
    "#        [-8, -7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6],\n",
    "#        [-9, -8, -7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5]])\n",
    "\n",
    "# Next: since we'er building an encoder, we \"mask\" the future by putting it to 0 i.e., we don't encode anything for the future \n",
    "# Also we make neg pos, just for convenience - doesn't really matter since it's all relative and consistent\n",
    "\n",
    "rel_pos = -rel_pos\n",
    "\n",
    "\n",
    "rel_pos = torch.max(rel_pos, torch.zeros_like(rel_pos))\n",
    "\n",
    "#rel_pos # For 10x20\n",
    "#tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [3, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [4, 3, 2, 1, 0, 0, 0, 0, 0, 0],\n",
    "#        [5, 4, 3, 2, 1, 0, 0, 0, 0, 0],\n",
    "#        [6, 5, 4, 3, 2, 1, 0, 0, 0, 0],\n",
    "#        [7, 6, 5, 4, 3, 2, 1, 0, 0, 0],\n",
    "#        [8, 7, 6, 5, 4, 3, 2, 1, 0, 0],\n",
    "#        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]])\n",
    "\n",
    "#rel_pos # For seq_len/query 10 and max_context_len/keys 15:\n",
    "#tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [8, 7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0]])                 \n",
    "\n",
    "# Now for the T5 modifications == the buckets\n",
    "\n",
    "# First half of the buckets are the actual tokens, so \"buckets\" with just one token in them\n",
    "\n",
    "num_token_buckets = num_buckets // 2    # This is 3 if num_buckets is 6, so 0, 1, 2 are exact and in single-item buckets\n",
    "\n",
    "# We're making the changes by applying masks on the matrix elements\n",
    "\n",
    "# First a mask that puts \"True\" on items that don't need to change (first 3), and False on all the others\n",
    "\n",
    "is_exact = rel_pos < num_token_buckets\n",
    "\n",
    "#is_exact\n",
    "# Last line is [False, False, False, False, False, False, False,  True,  True,  True]]) so that's True for ... 2, 1, 0]])  \n",
    "\n",
    "# Second mask: a mask that logaritmically puts more and more items in bins, up to max_distance.\n",
    "# This works by transforming the number to a max of num_buckets\n",
    "\n",
    "val_if_large = \\\n",
    "num_token_buckets + \\\n",
    "(torch.log(rel_pos.float() / num_token_buckets) / math.log(max_distance / num_token_buckets) * (num_buckets - num_token_buckets))\n",
    "\n",
    "# val_if_large\n",
    "# [5.4360, 5.3188, 5.1922, 5.0546, 4.9039, 4.7373, 4.5510, 4.3399, 4.0961, 3.8078, 3.4549, 3.0000, 2.3588, 1.2627,   -inf]]) \n",
    "\n",
    "# long() just converts to int\n",
    "val_if_large = val_if_large.long()\n",
    "\n",
    "#val_if_large\n",
    "# [5, 5, 5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, -9223372036854775808]]) # The last one is the smallest long int\n",
    "\n",
    "position_bucket_indices = torch.where(is_exact, rel_pos, val_if_large) # Where is_exact is True, put value from n, otherwise value from v_i_l\n",
    "\n",
    "position_bucket_indices -> 0, 1, 2 are always exact, from 3 on we start proper bucketing\n",
    "\n",
    "# Now we need to turn ALL of these items in positional embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

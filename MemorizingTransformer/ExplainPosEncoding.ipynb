{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a580795-be56-4b9e-97bc-1f942939d1e9",
   "metadata": {},
   "source": [
    "We will do relative positional encoding with modifications from the T5 paper\n",
    "\n",
    "The basic idea of relative pos encoding is using a matrix like this:\n",
    "\n",
    "tensor([[ 0, 1, 2, 3],\n",
    "        [-1, 0, 1, 2],\n",
    "        [-2,-1, 0, 1],\n",
    "        [-3,-2,-1, 0]])\n",
    "\n",
    "So that each positioning is encoded relative to the \"current token\" (query).\n",
    "\n",
    "Note that for decoder self-attention we want to use causal masking (meaning: we don't want to give ANY info about future tokens)\n",
    "so it will look like this:\n",
    "\n",
    "tensor([[ 0, 0, 0, 0],\n",
    "        [-1, 0, 0, 0],\n",
    "        [-2,-1, 0, 0],\n",
    "        [-3,-2,-1, 0]])\n",
    "\n",
    "and in this implementation we will not use negatives (does not matter) so it will be\n",
    "\n",
    "tensor([[ 0, 0, 0, 0],\n",
    "        [ 1, 0, 0, 0],\n",
    "        [ 2, 1, 0, 0],\n",
    "        [ 3, 2, 1, 0]])\n",
    "\n",
    "We'll then apply \"buckets\" where ranges will get the same pos. encoding, with the first 3 \"past\" tokens have their exact position, so it will look something like this for the last row (larger seq_len):\n",
    "\n",
    "[7,7,7,7,6,6,6,5,5,5,4,4,3,2,1,0,]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d804abc-9c1f-482b-87d2-468421999e7e",
   "metadata": {},
   "source": [
    "Some general info about nn.Embedding:\n",
    "\n",
    "Both nn.Linear and nn.Embedding will given you, in your example, a 3-dim vector. Thatâ€™s the whole point, i.e., to convert a token \n",
    "into an ideally meaningful vectors (i.e., a numeric and fix-sized representation of a word). The difference is w.r.t. the input\n",
    "nn.Linear expects a one-hot vector of the size of the vocabulary with the single 1 at the index representing the specific word.\n",
    "nn.Embedding just expects this index (and not a whole vector).\n",
    "\n",
    "However, if both nn.Linear and nn.Embedding would be initialized with the same weights, their outputs would be exactly the same.\n",
    "\n",
    "Yes, by default, the weights of both layers will be modified during the training process. In this respect, there are like any other \n",
    "layers in your network. However, you can tell the network not to modify the weights of any specific layer; I think it would look \n",
    "something like this:\n",
    "\n",
    "embedding = nn.Embedding(10, 3)\n",
    "embedding = weight.requires_grad = False\n",
    "\n",
    "This makes sense if you use pretrained word embeddings such as Word2Vec or Glove. If you initialize your weights randomly, you \n",
    "certainly want them to be modified during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85888592-7805-4502-93a8-3febc5fffb55",
   "metadata": {},
   "source": [
    "How is this matrix used:\n",
    "\n",
    "Basically, we'll wrap this all in a class that will be storing an embedding vector per position. In the video, the size of this vector\n",
    "corresponds to the number of heads, TBD why that is.\n",
    "\n",
    "Then, when we have the qv matrix with the dot products (scalars), well be adding the value for this attention head (from the vector) to \n",
    "the dot product. Since we're doing addition, this is all differentiable all the way to the vectors stored in the class, which is stored\n",
    "in a nn.Embedding object that is returned by the forward() method of our class - I think the latter is irrelevant though: it's \n",
    "differentiable since we're adding from the Embedding object itself, so backward() can backprop into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a886e0f-c91d-4357-b714-938c55504a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0],\n",
       "        [5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0],\n",
       "        [5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0],\n",
       "        [5, 5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0],\n",
       "        [5, 5, 5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "num_buckets = 6\n",
    "max_distance = 20    # Max sequence length - this will be 128 as per the paper\n",
    "seq_len = 15         # This is query length\n",
    "max_context_len = 15 # This is key length - normally same as query length but not for XL Trfrmrs where we concat keys as part of recurrency\n",
    "\n",
    "# Now we construct a matrix as per the above\n",
    "\n",
    "q_pos = torch.arange(seq_len, dtype=torch.long)               # Top row\n",
    "k_pos = torch.arange(max_context_len, dtype=torch.long) \n",
    "\n",
    "# Trick:\n",
    "#[0, 1, 2, 3] - [[0], == (via broadcasting) [[0, 1, 2, 3] - [[0, 0, 0, 0], == [[ 0, 1, 2, 3], \n",
    "#                [1],                        [0, 1, 2, 3]    [1, 1, 1, 1],     [-1, 0, 1, 2],\n",
    "#                [2],                        [0, 1, 2, 3]    [2, 2, 2, 2],     [-2,-1, 0, 1],\n",
    "#                [3]]                        [0, 1, 2, 3]]   [3, 3, 3, 3]]     [-3,-2,-1, 0]]\n",
    "\n",
    "# So we need to convert q_pos to a column vector:\n",
    "q_pos = q_pos.reshape(q_pos.shape[0], 1)\n",
    "\n",
    "rel_pos = k_pos - q_pos\n",
    "#rel_pos # With seq_len 10 for query and max_context_len 15 for (concatenated) keys this gives:\n",
    "# Query goes \"up/down\" since we only have the current sequence, but we match it with a concat of keys for recurrence ->>\n",
    "#tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14],\n",
    "#        [-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
    "#        [-2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
    "#        [-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
    "#        [-4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
    "#        [-5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
    "#        [-6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
    "#        [-7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7],\n",
    "#        [-8, -7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6],\n",
    "#        [-9, -8, -7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5]])\n",
    "\n",
    "# Next: since we'er building an encoder, we \"mask\" the future by putting it to 0 i.e., we don't encode anything for the future \n",
    "# Also we make neg pos, just for convenience - doesn't really matter since it's all relative and consistent\n",
    "\n",
    "rel_pos = -rel_pos\n",
    "\n",
    "\n",
    "rel_pos = torch.max(rel_pos, torch.zeros_like(rel_pos))\n",
    "\n",
    "#rel_pos # For 10x20\n",
    "#tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [3, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [4, 3, 2, 1, 0, 0, 0, 0, 0, 0],\n",
    "#        [5, 4, 3, 2, 1, 0, 0, 0, 0, 0],\n",
    "#        [6, 5, 4, 3, 2, 1, 0, 0, 0, 0],\n",
    "#        [7, 6, 5, 4, 3, 2, 1, 0, 0, 0],\n",
    "#        [8, 7, 6, 5, 4, 3, 2, 1, 0, 0],\n",
    "#        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]])\n",
    "\n",
    "#rel_pos # For seq_len/query 10 and max_context_len/keys 15:\n",
    "#tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [8, 7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0]])                 \n",
    "\n",
    "# Now for the T5 modifications == the buckets\n",
    "\n",
    "# First half of the buckets are the actual tokens, so \"buckets\" with just one token in them\n",
    "\n",
    "num_token_buckets = num_buckets // 2    # This is 3 if num_buckets is 6, so 0, 1, 2 are exact and in single-item buckets\n",
    "\n",
    "# We're making the changes by applying masks on the matrix elements\n",
    "\n",
    "# First a mask that puts \"True\" on items that don't need to change (first 3), and False on all the others\n",
    "\n",
    "is_exact = rel_pos < num_token_buckets\n",
    "\n",
    "#is_exact\n",
    "# Last line is [False, False, False, False, False, False, False,  True,  True,  True]]) so that's True for ... 2, 1, 0]])  \n",
    "\n",
    "# Second mask: a mask that logaritmically puts more and more items in bins, up to max_distance.\n",
    "# This works by transforming the number to a max of num_buckets\n",
    "\n",
    "val_if_large = \\\n",
    "num_token_buckets + \\\n",
    "(torch.log(rel_pos.float() / num_token_buckets) / math.log(max_distance / num_token_buckets) * (num_buckets - num_token_buckets))\n",
    "\n",
    "# val_if_large\n",
    "# [5.4360, 5.3188, 5.1922, 5.0546, 4.9039, 4.7373, 4.5510, 4.3399, 4.0961, 3.8078, 3.4549, 3.0000, 2.3588, 1.2627,   -inf]]) \n",
    "\n",
    "# long() just converts to int\n",
    "val_if_large = val_if_large.long()\n",
    "\n",
    "# print(val_if_large) -> for 15 x 15 q x k\n",
    "# [5, 5, 5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, -9223372036854775808]]) # The last one is the smallest long int\n",
    "\n",
    "# The below seems to be extra precaution to make sure that any number in here is never larger than (num_buckets -1).\n",
    "val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "\n",
    "position_bucket_indices = torch.where(is_exact, rel_pos, val_if_large) # Where is_exact is True, put value from n, otherwise value from v_i_l\n",
    "\n",
    "position_bucket_indices #-> 0, 1, 2 are always exact, from 3 on we start proper bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d0095-f7c7-4bac-a441-6022f412a9e8",
   "metadata": {},
   "source": [
    "Note on Embedding layer: this is just like a Linear layer, only it's optimized for sparse inputs and one-hot encoding.\n",
    "\n",
    "--\n",
    "\n",
    "Now we need to turn ALL of these items in positional embeddings.\n",
    "\n",
    "In our reference implementation, this is done like this:\n",
    "\n",
    "self.position_embeddings = nn.Embedding(self.seq_len, config['embed_size'])\n",
    "\n",
    "Where embed_size matches the embed_size of the token embeddings, so that we can add the vectors up\n",
    "\n",
    "In our reference implementation, we do absolute, not relatlive, and the Embedding layers has to \"learn\" a vector of weights\n",
    "for each of the absolute positions.\n",
    "\n",
    "Here, there will be one such vector for each relative position i.e., 0, 1, ..., 5 -> 6 vectors, where each vector has size \"num_heads\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc642ad4-f633-4670-a48d-e8328fe3f228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 15, 15])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_attn_heads = 4\n",
    "\n",
    "rel_pos_embeddings = nn.Embedding(num_buckets, num_attn_heads)\n",
    "#rel_pos_embeddings\n",
    "#Embedding(6, 4)\n",
    "\n",
    "#print(rel_pos_embeddings.weight)\n",
    "#tensor([[-0.1299,  1.1240, -1.4298,  0.9117],   <--- \n",
    "#        [-0.1299,  1.9153, -1.3115,  0.3001],\n",
    "#        [ 1.4696,  0.7510,  0.6996, -1.7691],\n",
    "#        [-1.2145, -1.5550, -2.3301, -1.0363],\n",
    "#        [ 0.7550, -1.3407,  1.7770,  0.6528],\n",
    "#        [ 0.1610,  2.0491, -2.0410,  0.7201]], requires_grad=True)\n",
    "\n",
    "# Now we need to add a num_attn_heads sized vector to the token embeddings in the query-key matrics, which means that in the\n",
    "# position_bucket_indices matrix we need to replace each number by corresponding vector in position_embeddings. This way we can then\n",
    "# add up the two query-key sized matrices element by element (overlay vector addition).\n",
    "\n",
    "q_k_rel_pos_embeddings = rel_pos_embeddings(position_bucket_indices)\n",
    "\n",
    "#q_k_rel_pos_embeddings.shape\n",
    "#torch.Size([15, 15, 4])   -> (seq_len for queries, max_context_len for keys, size of pos. embedding vector)\n",
    "\n",
    "# Note: in the paper, they have a (seq_len, max_context_len) matrix for each head, so in each head they only use one of the\n",
    "# num_attn_heads (4 in this case) \"slices\". So we need to bring the last dimenion to the front (to easily get to a slice).\n",
    "\n",
    "# So we need to go from (seq_len, max_context_len, num_attn_heads) -> (batch, num_attn_heads, seq_len, max_context_len)\n",
    "\n",
    "\n",
    "#print(q_k_rel_pos_embeddings.shape)  # -> torch.Size([10, 20, 4]) so (seq_len, max_content_len, num_attn_heads)\n",
    "\n",
    "#print(q_k_rel_pos_embeddings.transpose(0,2).shape) # -> torch.Size([4, 20, 10]) so (num_attn_heads, max_content_len, seq_len)\n",
    "\n",
    "# The above is what the video does, and it does not seem to lead to the correct dimensions, (num_attn_heads, seq_len, max_content_len)\n",
    "# which would be (4, 10, 20)\n",
    "\n",
    "q_k_rel_pos_embeddings = q_k_rel_pos_embeddings.transpose(-1,-2).transpose(0,1) # -> (4, 10, 20)\n",
    "\n",
    "\n",
    "#print(rel_pos_embeddings.weight)\n",
    "#print(q_k_rel_pos_embeddings[0])\n",
    "#\n",
    "#tensor([[-0.6612, -1.0359,  0.1423, -0.1087], <- This is the num_attn_head size vector for rel pos 0\n",
    "#        [ 0.3478,  0.1189,  0.9446, -0.2021], <- \"\"                                                1\n",
    "#        [ 0.4463, -0.3085,  0.4393, -0.6813], <- \"\"                                                2\n",
    "#        [-0.1137, -0.1136, -1.1700, -0.3337], <- \"\"                                                3\n",
    "#        [-0.6651, -1.0278, -0.1142,  0.3004], <- \"\"                                                4\n",
    "#        [-1.2078,  0.9238, -0.5395,  1.7939]], requires_grad=True) <- \"\"                           5\n",
    "#\n",
    "# We are taking the first vertical slice here, so 0 -> -0.6612, 1 -> 0.3478, etc.\n",
    "#\n",
    "#tensor([[-0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612],\n",
    "#        [ 0.3478, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612],\n",
    "#        [ 0.4463,  0.3478, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612],\n",
    "#        [-0.1137,  0.4463,  0.3478, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612],\n",
    "#        [-0.1137, -0.1137,  0.4463,  0.3478, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612],\n",
    "#        [-0.1137, -0.1137, -0.1137,  0.4463,  0.3478, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612],\n",
    "#        [-0.6651, -0.1137, -0.1137, -0.1137,  0.4463,  0.3478, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612],\n",
    "#        [-0.6651, -0.6651, -0.1137, -0.1137, -0.1137,  0.4463,  0.3478, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612],\n",
    "#        [-0.6651, -0.6651, -0.6651, -0.1137, -0.1137, -0.1137,  0.4463,  0.3478,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612],\n",
    "#        [-0.6651, -0.6651, -0.6651, -0.6651, -0.1137, -0.1137, -0.1137,  0.4463,\n",
    "#          0.3478, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612, -0.6612,\n",
    "#         -0.6612, -0.6612, -0.6612, -0.6612]], grad_fn=<SelectBackward0>)\n",
    "\n",
    "# Now add the batch dimension so that it becomes (1, 4, 10, 20) so (1, num_attn_heads, seq_len, max_context_len)\n",
    "\n",
    "q_k_rel_pos_embeddings = q_k_rel_pos_embeddings.unsqueeze(0)\n",
    "\n",
    "q_k_rel_pos_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da21c1db-420b-4a16-89b4-b0d41f3b72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class version\n",
    "\n",
    "# Again this returns a tensor object of size (1, num_attn_heads, queries, keys) where each attn head is \n",
    "# supposed to one \"layer\" which is queries x keys\n",
    "\n",
    "class RelativePosition(nn.Module):\n",
    "\n",
    "    def __init__(self, scaling_factor, num_buckets, max_distance, num_attn_heads):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.rel_pos_embeddings = nn.Embedding(num_buckets, num_attn_heads)\n",
    "\n",
    "    # Which each forward pass we provide our \"latest\" tensor object, updated in the previous backward() pass\n",
    "    \n",
    "    def forward(self, seq_len, max_context_len):     # These app. can change depending on the layer - check this.\n",
    "        \n",
    "        q_pos = torch.arange(seq_len, dtype=torch.long)               \n",
    "        k_pos = torch.arange(max_context_len, dtype=torch.long)\n",
    "        q_pos = q_pos.reshape(q_pos.shape[0], 1)\n",
    "        rel_pos = k_pos - q_pos\n",
    "        rel_pos = -rel_pos\n",
    "        rel_pos = torch.max(rel_pos, torch.zeros_like(rel_pos))\n",
    "        # We have rel_pos, now we need is_exact matrix and val_if_large matrix\n",
    "        \n",
    "        num_token_buckets = self.num_buckets // 2\n",
    "        is_exact = rel_pos < num_token_buckets\n",
    "\n",
    "        val_if_large = \\\n",
    "        num_token_buckets + \\\n",
    "        (torch.log(rel_pos.float() / num_token_buckets) / math.log(self.max_distance / num_token_buckets) * (self.num_buckets - num_token_buckets))\n",
    "        val_if_large = val_if_large.long()\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "\n",
    "        position_bucket_indices = torch.where(is_exact, rel_pos, val_if_large)\n",
    "\n",
    "        q_k_rel_pos_embeddings = self.rel_pos_embeddings(position_bucket_indices)\n",
    "\n",
    "        q_k_rel_pos_embeddings = q_k_rel_pos_embeddings.transpose(-1,-2).transpose(0,1)\n",
    "        \n",
    "        q_k_rel_pos_embeddings = q_k_rel_pos_embeddings.unsqueeze(0)\n",
    "\n",
    "        return q_k_rel_pos_embeddings * self.scaling_factor # This is new in the class.        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6f8265f-4701-4e89-90d8-0a04ceaa6133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 15, 15])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relpos = RelativePosition(1, 6, 20, 4)\n",
    "relpos(15,15).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

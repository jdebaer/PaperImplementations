{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2712dd16-cf0a-4dc9-9e69-4d30f092ddda",
   "metadata": {},
   "source": [
    "torch.tensor vs. torch.Tensor:\n",
    "\n",
    "torch.tensor infers the dtype automatically, while torch.Tensor is an alias for torch.FloatTensor and returns a torch.FloatTensor. I would recommend to stick to torch.tensor, which also has arguments like dtype\n",
    "\n",
    "torch.tensor/as_tensor vs. torch.from_numpy:  \n",
    "\n",
    "from_numpy() inherits the dtype of the ndarray. On the other hand, torch.Tensor is an alias for torch.FloatTensor and can only return a FloatTensor. Therefore, if you pass int64 array to torch.Tensor, output tensor is FloatTensor and they wouldn't share the storage. torch.from_numpy gives you torch.LongTensor as expected and the storage will be shared.\n",
    "\n",
    "torch.tensor vs. torch.as_tensor:\n",
    "torch.tensor always copies the data. For example, torch.tensor(x) is equivalent to x.clone().detach().\n",
    "\n",
    "torch.as_tensor tries to avoid copying the data. One of the cases where as_tensor avoids copying the data is if the original data is a numpy array.\n",
    "\n",
    "When a numpy array and a tensor share the storage, then changing one will change the other.\n",
    "\n",
    "Bottom line:\n",
    "\n",
    "keys_and_values = np.random.rand(16 * 512, 3, 2, 10).astype('float32')\n",
    "\n",
    "keys_and_values = torch.tensor(keys_and_values)  --> This copies the data, no sharing\n",
    "\n",
    "keys_and_values = torch.as_tensor(keys_and_values) -> This does NOT copy the data, sharing\n",
    "\n",
    "keys_and_values = torch.from_numpy(keys_and_values) -> This does NOT copy the data, sharing\n",
    "\n",
    "############ Other way round ##############\n",
    "\n",
    "\n",
    "Tensor.numpy(*, force=False) → numpy.ndarray\n",
    "Returns the tensor as a NumPy ndarray.\n",
    "\n",
    "If force is False (the default), the conversion is performed only if the tensor is on the CPU, does not require grad, does not have its conjugate bit set, and is a dtype and layout that NumPy supports. The returned ndarray and the tensor will share their storage, so changes to the tensor will be reflected in the ndarray and vice versa.\n",
    " )\n",
    "If force is True this is equivalent to calling t.detach().cpu().resolve_conj().resolve_neg().numpy(). If the tensor isn’t on the CPU or the conjugate or negative bit is set, the tensor won’t share its storage with the returned ndarray. Setting force to True can be a useful shorthand.\n",
    "\n",
    "Bottom line: you should only call numpy() if you have already moved the Tensor to the CPU !!!!!!!!! and they will SHARE the storage !!!\n",
    "\n",
    "Note that cpu() already creates a copy of the tensor if the tensor was on the GPU (which will typically be the case)\n",
    "\n",
    "To create a copy of a tensor, always use .detach().clone() and it will create a copy whereever it is (CPU or GPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d791d-836b-4eb6-82ce-5ac6e8a20573",
   "metadata": {},
   "source": [
    "Quick note one np.random.rand vs. np.random.random, latter is alias for np.random.random_samples\n",
    "With numpy.random.rand, the length of each dimension of the output array is a separate argument. \n",
    "With numpy.random.random_sample, the shape argument is a single tuple.\n",
    "For example, to create an array of samples with shape (3, 5), you can write\n",
    "sample = np.random.rand(3, 5)\n",
    "or\n",
    "sample = np.random.random_sample((3, 5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7108d-60b7-440e-b783-7de25c67e1a4",
   "metadata": {},
   "source": [
    "When selecting a vector DB, consider the following:\n",
    "\n",
    "What operations do we need?\n",
    "- Add?\n",
    "- Store?\n",
    "- Search?\n",
    "- Remove?\n",
    "- Reconstruct?\n",
    "\n",
    "Frequency of each operation?\n",
    "Accuracy vs. speed vs. required mem footprint?\n",
    "Size of index (how many vectors will we be storing)?\n",
    "Size of query (how many vectors are we querying with)?\n",
    "GPU or CPU?\n",
    "Is retraining the index with new additions required?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49513ce4-a693-447c-9861-ec9bd8041f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: mem_dim CAN BE WHATEVER DIM, this can be embed_dim or this can be the attn head dim, depends how you implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02fb918f-26cc-4aae-a1d7-1eba2aa75ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# We need a simply vector DB where we can search, add and remove (forget) - Faiss can do all this\n",
    "\n",
    "# The size of the vectors we'll be storing.\n",
    "mem_dim = 64\n",
    "\n",
    "# The max amount of (key,value) pairs we're storing in our memory\n",
    "max_mems = 10000\n",
    "\n",
    "# index object is basically the DB - L2 is the distance metric (Euclidian). Faiss types trade off speed/accuracy/memory footprint.\n",
    "# FlatL2 supports add/remove/search so it will do.\n",
    "\n",
    "index = faiss.IndexFlatL2(mem_dim)\n",
    "\n",
    "# For each of the 10, we want the 2 most similar ones in the index (via Euclidian).\n",
    "top_k = 2\n",
    "\n",
    "# Anything below this point in this cell is the initial explanation, comment out to teach, before you go to next cell\n",
    "#\n",
    "## Let's ADD some fake vectors\n",
    "#vectors = np.random.random((10000,mem_dim)).astype('float32')\n",
    "#index.add(vectors)\n",
    "##index.ntotal -> 10000\n",
    "#\n",
    "## Let's REMOVE some vectors\n",
    "##index.remove_ids(np.arange(10)) # Remove vectors with id's 0 through 9\n",
    "##index.ntotal -> 9990\n",
    "#\n",
    "## Let's do a query\n",
    "#query_vectors = np.random.random((10,mem_dim)).astype('float32')     # Batch of 10 query vectors\n",
    "#\n",
    "#\n",
    "#\n",
    "#distances, ids = index.search(query_vectors, top_k)\n",
    "#\n",
    "##distances\n",
    "##array([[5.5422297, 5.679163 ],    <<<<<< distances for 2 closest vectors for first vector in our search\n",
    "##       [5.946422 , 5.969053 ],\n",
    "##       [5.812578 , 6.1550074],\n",
    "##       [5.0106854, 5.0456457],\n",
    "##       [5.7170763, 5.8991313],\n",
    "##       [5.5098534, 5.654152 ],\n",
    "##       [6.250798 , 6.3652353],\n",
    "##       [5.3992786, 5.7259717],\n",
    "##       [5.371162 , 5.6683345],\n",
    "##       [5.3517475, 5.617537 ]], dtype=float32)\n",
    "#\n",
    "##ids\n",
    "##array([[1285, 8094],             <<<<<< ids of the 2 closest vectors (0 -> 9999) here\n",
    "##       [4660, 8888],\n",
    "##       [6929, 4617],\n",
    "##       [9606,   99],\n",
    "##       [3304, 3644],\n",
    "##       [6760,  265],\n",
    "##       [8001, 1974],\n",
    "##       [9486, 2416],\n",
    "##       [5023, 2839],\n",
    "##       [4886,  580]])\n",
    "#\n",
    "##distances, ids, vectors = index.search_and_reconstruct(query_vectors, top_k)\n",
    "### This also contains the actual matching vectors\n",
    "##vectors.shape\n",
    "###(10, 2, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ce3ab-99ef-4c57-b5f6-81802f0aff23",
   "metadata": {},
   "source": [
    "In the last layer, which is the kNN-augmented layer, we'll be searching in FAISS using the key vectors (which will be the same as the\n",
    "query vectors (unless there is recurrence, in which case we have more keys than queries). For each key vector, we'll search the top_k\n",
    "matching key vectors that we have in memory.\n",
    "\n",
    "Now we also need the corresponding value vectors, so we need to store these as well. We'll do vector similarity matching with the keys\n",
    "so the keys must go in a vector DB, but we can store the matching values in a regular DB as long as we can retrieve each value that \n",
    "corresponds with a matching key (from the vector DB).\n",
    "\n",
    "In this case we're going to store it on disk with a memmap numpy array. This is nice because it allows us to talk to our data using \n",
    "the numpy interface (just as if it would be all in mem) - but it's backed by a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a2d3ff-c810-4239-b5d3-e3364a2e1ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_filepath = \"./numpy.memmap\"\n",
    "db = np.memmap(db_filepath, mode='w+', dtype=np.float32, shape = (max_mems,2,mem_dim))\n",
    "\n",
    "# db[1] = np.random.rand(1,2,mem_dim) <<<<<< store 2 vectors in position 1 (second position)\n",
    "# db\n",
    "# memmap([[[0.        , 0.        , 0.        , ..., 0.        ,\n",
    "#          0.        , 0.        ],\n",
    "#         [0.        , 0.        , 0.        , ..., 0.        ,\n",
    "#          0.        , 0.        ]],\n",
    "#\n",
    "#        [[0.7254081 , 0.07150084, 0.92323035, ..., 0.28860936,\n",
    "#          0.20353778, 0.72763807],\n",
    "#         [0.00908635, 0.2388858 , 0.34646714, ..., 0.73859435,\n",
    "#          0.35705045, 0.8246553 ]],\n",
    "#\n",
    "#        [[0.        , 0.        , 0.        , ..., 0.        ,\n",
    "#          0.        , 0.        ],\n",
    "#         [0.        , 0.        , 0.        , ..., 0.        ,\n",
    "#          0.        , 0.        ]],\n",
    "#\n",
    "#        ...,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288bee68-1db2-4694-be99-77b1903323f5",
   "metadata": {},
   "source": [
    "Now we put this all in a class - object must be able to:\n",
    "- add to index and database\n",
    "- query the index\n",
    "- retrieve in database based on query results\n",
    "- remove from index and database (\"forgetting the oldest memories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028c21aa-efa9-4d77-9ebf-3c680c8dc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDING\n",
    "\n",
    "# Let's first create some fake data that we can push through - and to understand requirements as well\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "keys = np.random.rand(batch_size, 512, mem_dim).astype('float32')    # 512 is seq_len\n",
    "\n",
    "# Now we want to feed in the values as well, so we create an extra dimension\n",
    "\n",
    "keys_and_values = np.random.rand(batch_size, 512, 2, mem_dim).astype('float32')\n",
    "\n",
    "# Say we have 16 as batch_size => 16 * 512 vector PAIRS (key and value) to into the db, while 16*512 key vectors go into the index\n",
    "# Note: this means we're storing a memory for each of the tokens in the sequence - but we only really use the last token to predict the\n",
    "# next one ... is this right?\n",
    "\n",
    "# Let's slam batch_size and seq_len together:\n",
    "# Alternative:\n",
    "# keys_and_values = keys_and_values.flatten(0,1) apparently -> test this\n",
    "\n",
    "keys_and_values = keys_and_values.reshape(-1, 2, mem_dim)         # -1 means infer the first dimension\n",
    "\n",
    "# Now for the index we just need to store the keys, so let's grab those\n",
    "\n",
    "keys = keys_and_values[:,0,:]\n",
    "# Btw:\n",
    "# keys.shape\n",
    "# (512, 64)\n",
    "\n",
    "index.add(keys)\n",
    "# If you ever get error here on non contiguous array: index.add(np.ascontiguousarray(keys))  -> this is FAISS thing, not related to tensors\n",
    "\n",
    "# For the db numpy array we have to keep track of where we are ourselves\n",
    "\n",
    "db_offset = 0 # initialize once at the level of the object\n",
    "\n",
    "# Then each time the add function is called:\n",
    "\n",
    "num_added = keys_and_values.shape[0]\n",
    "\n",
    "indexes_added = (np.arange(num_added) + db_offset)\n",
    "\n",
    "db_offset = db_offset + num_added\n",
    "\n",
    "db[indexes_added] = keys_and_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc74ed0-99b1-4147-9e54-f776e602dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERYING THE INDEX\n",
    "\n",
    "# Create fake vectors that we want to query with\n",
    "\n",
    "queries = np.random.rand(batch_size, 512, mem_dim).astype('float32')    # We're going to query with EVERY token from every sequence in the batch \n",
    "\n",
    "# Again we slam batch_size and seq_len together\n",
    "queries = queries.reshape(-1, mem_dim)\n",
    "\n",
    "# Let's try a search\n",
    "\n",
    "distances, ids = index.search(queries, top_k)\n",
    "\n",
    "# Since we store things in the db in the same order as for the index, we can simply query the db using the returned ids (which are indexes).\n",
    "\n",
    "result_keys_and_values = db[ids]\n",
    "\n",
    "\n",
    "#result_keys_and_values.shape # -> (1024, 2, 2, 64) for batch_size x seq_len, top_k, and then the query and the key vector\n",
    "\n",
    "# Now when we return this to the layer, we want to restore the batch and seq_len dimension.\n",
    "\n",
    "result_keys_and_values = result_keys_and_values.reshape(batch_size, 512, top_k, 2, mem_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85fee11-7431-4673-a75e-8b09a5d954ca",
   "metadata": {},
   "source": [
    "REMOVAL (forgetting)\n",
    "\n",
    "In the paper they have one memory for each batch dimension (the dimension in which a paper is fed in). So if batch_size is 8 then\n",
    "there are 8 memories (index and DB). This makes the whole thing more efficient since the best hits will come from the same paper\n",
    "anyway. In this implementation we'll just do a shared memory across all batches.\n",
    "\n",
    "Also in our implementation we're going to clear the database after each batch. In the batch dimension there are 1O chunks or 512\n",
    "tokens and we'll use the memory as we work through those 10, but after that we start with an empty memory again.\n",
    "\n",
    "So we're implementing a clear() function that simply clears out the index and the DB.\n",
    "\n",
    "So there's no real \"forgetting\" here, it's more like restarting from scratch with each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b397f7b1-054f-46f2-9616-bb82cd2e8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we need to add all of the above in a class form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee695dc2-eec9-42e7-9714-d9025337bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Memory:\n",
    "\n",
    "    def __init__(self, mem_dim=64, max_mems=10000):\n",
    "\n",
    "        self.index = faiss.IndexFlatL2(mem_dim)\n",
    "        db_filepath = \"./numpy.memmap\"          \n",
    "        self.db = np.memmap(db_filepath, mode='w+', dtype=np.float32, shape = (max_mems,2,mem_dim))\n",
    "\n",
    "        self.db_offset = 0\n",
    "\n",
    "    def add(self, keys_and_values): # keys_and_values is (b_s, seq_len, 2, mem_dim) so for each token the key and the value vectors\n",
    "\n",
    "        # Note: what comes in in a tensor\n",
    "\n",
    "        # Vid: keys_and_values = keys_and_values.flatten(0,1)\n",
    "        # Note that reshape reuses the buffer because the order of the elements is not changed.\n",
    "        keys_and_values = keys_and_values.reshape(-1, 2, mem_dim) # Slam batch and seq_len together so (b_s * seq_len, 2, mem_dim) now\n",
    "\n",
    "        # Vid: keys, values = keys_and_values.unbind(mem_dim=-2)\n",
    "        keys = keys_and_values[:,0,:]\n",
    "\n",
    "        # Vid does this first: \n",
    "        #keys = np.ascontiguousarray(keys.numpy())\n",
    "        # The ascontiguous should not be needed, but at the min the below should throw an error since FAISS only supports numpy, so:\n",
    "        keys = keys.numpy()\n",
    "\n",
    "        # Note that with the above, the tensor and the numpy array share storage. This should be ok since we already should have \n",
    "        # created a copy with cpu() before getting to this point.\n",
    "        \n",
    "        self.index.add(keys)\n",
    "        \n",
    "        # Now the DB\n",
    "\n",
    "        num_added = keys_and_values.shape[0]\n",
    "\n",
    "        indexes_added = (np.arange(num_added) + self.db_offset)\n",
    "\n",
    "        self.db[indexes_added] = keys_and_values # Automatic conversion from Tensor (right) to Numpy array (left)\n",
    "\n",
    "        self.db_offset = self.db_offset + num_added\n",
    "\n",
    "        self.db.flush() # needed?\n",
    "\n",
    "    def query(self, queries, top_k=2):  # queries is (b_s, seq_len, mem_dim)\n",
    "\n",
    "        # Need to fetch b_s and seq_len because we need to restore to them later:\n",
    "\n",
    "        b_s = queries.shape[0]\n",
    "        seq_len = queries.shape[1]\n",
    "\n",
    "        queries = queries.reshape(-1, mem_dim) # queries is now (b_s * seq_len, mem_dim)\n",
    "\n",
    "        # queries = np.ascontiguousarray(queries.numpy()) \n",
    "        # ascontiguous() should not be needed but converting Tensor to Numpy array should be as FAISS only supports numpy:\n",
    "        queries = queries.numpy()\n",
    "\n",
    "        # Same remark here, query tensors will already be copies because cpy() should already have been called.\n",
    "        \n",
    "        distances, ids = self.index.search(queries, top_k)\n",
    "        \n",
    "        result_keys_and_values = self.db[ids]\n",
    "\n",
    "        # Now restore the b_s and seq_len dimesions\n",
    "\n",
    "        result_keys_and_values = result_keys_and_values.reshape(b_s, seq_len, top_k, 2, mem_dim)\n",
    "        \n",
    "        # Now we need to convert Numpy back to Tensor\n",
    "\n",
    "        #result_keys_and_values = torch.tensor(result_keys_and_values) <<<< this one is giving issues, maybe due to copying\n",
    "        result_keys_and_values = torch.from_numpy(result_keys_and_values)\n",
    "\n",
    "        # Regarding the above: tensor and numpy array share the storage but assumption is this is ok because to() will create copy\n",
    "        \n",
    "        # Note that in the vid the final reshape is done AFTER the conversion to Tensor, like this:\n",
    "        # result_keys_and_values = torch.unflatten(result_keys_and_values, 0, (b_s, seq_len))\n",
    "        \n",
    "        return result_keys_and_values # (b_s, seq_len, top_k, 2, mem_dim])\n",
    "    \n",
    "    def clear():\n",
    "\n",
    "        self.index.reset()\n",
    "        self.db[:] = 0       # Optional but clean\n",
    "        self.db_offset = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86154f0-88f3-4ce7-adff-07e247364a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 3, 2, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "batch_size = 8\n",
    "mem_dim = 10\n",
    "segments = 10\n",
    "seq_len = 512\n",
    "max_mems = batch_size * segments * seq_len\n",
    "\n",
    "mem = Memory(mem_dim=mem_dim, max_mems=max_mems)\n",
    "\n",
    "#keys_and_values = np.random.rand(batch_size, 512, 2, mem_dim).astype('float32')\n",
    "# We need the torch version of this:\n",
    "keys_and_values = torch.randn(batch_size, seq_len, 2, mem_dim)    # Default type is torch.FloatTensor == 32-bit float\n",
    "\n",
    "queries = torch.randn(batch_size, seq_len, mem_dim)\n",
    "\n",
    "# 1. Add keys and values to the memory\n",
    "\n",
    "mem.add(keys_and_values)\n",
    "\n",
    "#mem.index.ntotal -> 8192\n",
    "#mem.db[8191] -> at this location we indeed have 2 vectors stored:\n",
    "#memmap([[ 0.5223559 ,  0.82290304, -0.24579963,  0.33095127, -0.19270769,\n",
    "#         -0.09946475, -1.3664232 ,  0.87740934, -1.7536417 ,  1.4747871 ],\n",
    "#        [ 0.30017638, -1.134407  ,  1.0770321 ,  0.76789665, -0.6984901 ,\n",
    "#         -0.49128056,  0.8477661 , -0.98776984,  0.02807933, -0.5398302 ]],\n",
    "#       dtype=float32)\n",
    "\n",
    "result_keys_and_values = mem.query(queries,3)\n",
    "\n",
    "result_keys_and_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07264a89-70b8-407a-9667-c40914ad8c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 3, 2, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "keys_and_values = np.random.rand(16 * 512, 3, 2, 10).astype('float32')\n",
    "#keys_and_values = np.random.rand(1, 5, 2, 10).astype('float32')\n",
    "\n",
    "#keys_and_values = torch.tensor(keys_and_values)\n",
    "keys_and_values = torch.from_numpy(keys_and_values)\n",
    "keys_and_values.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680a6215-48b0-4af2-b59f-f4168e232de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From here got to stage 6, which is stage 5 but with PosEncoding and Memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

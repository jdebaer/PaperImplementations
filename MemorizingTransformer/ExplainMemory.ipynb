{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341d791d-836b-4eb6-82ce-5ac6e8a20573",
   "metadata": {},
   "source": [
    "Quick note one np.random.rand vs. np.random.random, latter is alias for np.random.random_samples\n",
    "With numpy.random.rand, the length of each dimension of the output array is a separate argument. \n",
    "With numpy.random.random_sample, the shape argument is a single tuple.\n",
    "For example, to create an array of samples with shape (3, 5), you can write\n",
    "sample = np.random.rand(3, 5)\n",
    "or\n",
    "sample = np.random.random_sample((3, 5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7108d-60b7-440e-b783-7de25c67e1a4",
   "metadata": {},
   "source": [
    "When selecting a vector DB, consider the following:\n",
    "\n",
    "What operations do we need?\n",
    "- Add?\n",
    "- Store?\n",
    "- Search?\n",
    "- Remove?\n",
    "- Reconstruct?\n",
    "\n",
    "Frequency of each operation?\n",
    "Accuracy vs. speed vs. required mem footprint?\n",
    "Size of index (how many vectors will we be storing)?\n",
    "Size of query (how many vectors are we querying with)?\n",
    "GPU or CPU?\n",
    "Is retraining the index with new additions required?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02fb918f-26cc-4aae-a1d7-1eba2aa75ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# We need a simply vector DB where we can search, add and remove (forget) - Faiss can do all this\n",
    "\n",
    "# The size of the vectors we'll be storing.\n",
    "dim = 64\n",
    "\n",
    "# The max amount of (key,value) pairs we're storing in our memory\n",
    "max_mems = 10000\n",
    "\n",
    "# index object is basically the DB - L2 is the distance metric (Euclidian). Faiss types trade off speed/accuracy/memory footprint.\n",
    "# FlatL2 supports add/remove/search so it will do.\n",
    "\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "# For each of the 10, we want the 2 most similar ones in the index (via Euclidian).\n",
    "top_k = 2\n",
    "\n",
    "# Anything below this point in this cell is the initial explanation, comment out to teach, before you go to next cell\n",
    "#\n",
    "## Let's ADD some fake vectors\n",
    "#vectors = np.random.random((10000,dim)).astype('float32')\n",
    "#index.add(vectors)\n",
    "##index.ntotal -> 10000\n",
    "#\n",
    "## Let's REMOVE some vectors\n",
    "##index.remove_ids(np.arange(10)) # Remove vectors with id's 0 through 9\n",
    "##index.ntotal -> 9990\n",
    "#\n",
    "## Let's do a query\n",
    "#query_vectors = np.random.random((10,dim)).astype('float32')     # Batch of 10 query vectors\n",
    "#\n",
    "#\n",
    "#\n",
    "#distances, ids = index.search(query_vectors, top_k)\n",
    "#\n",
    "##distances\n",
    "##array([[5.5422297, 5.679163 ],    <<<<<< distances for 2 closest vectors for first vector in our search\n",
    "##       [5.946422 , 5.969053 ],\n",
    "##       [5.812578 , 6.1550074],\n",
    "##       [5.0106854, 5.0456457],\n",
    "##       [5.7170763, 5.8991313],\n",
    "##       [5.5098534, 5.654152 ],\n",
    "##       [6.250798 , 6.3652353],\n",
    "##       [5.3992786, 5.7259717],\n",
    "##       [5.371162 , 5.6683345],\n",
    "##       [5.3517475, 5.617537 ]], dtype=float32)\n",
    "#\n",
    "##ids\n",
    "##array([[1285, 8094],             <<<<<< ids of the 2 closest vectors (0 -> 9999) here\n",
    "##       [4660, 8888],\n",
    "##       [6929, 4617],\n",
    "##       [9606,   99],\n",
    "##       [3304, 3644],\n",
    "##       [6760,  265],\n",
    "##       [8001, 1974],\n",
    "##       [9486, 2416],\n",
    "##       [5023, 2839],\n",
    "##       [4886,  580]])\n",
    "#\n",
    "##distances, ids, vectors = index.search_and_reconstruct(query_vectors, top_k)\n",
    "### This also contains the actual matching vectors\n",
    "##vectors.shape\n",
    "###(10, 2, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ce3ab-99ef-4c57-b5f6-81802f0aff23",
   "metadata": {},
   "source": [
    "In the last layer, which is the kNN-augmented layer, we'll be searching in FAISS using the key vectors (which will be the same as the\n",
    "query vectors (unless there is recurrence, in which case we have more keys than queries). For each key vector, we'll search the top_k\n",
    "matching key vectors that we have in memory.\n",
    "\n",
    "Now we also need the corresponding value vectors, so we need to store these as well. We'll do vector similarity matching with the keys\n",
    "so the keys must go in a vector DB, but we can store the matching values in a regular DB as long as we can retrieve each value that \n",
    "corresponds with a matching key (from the vector DB).\n",
    "\n",
    "In this case we're going to store it on disk with a memmap numpy array. This is nice because it allows us to talk to our data using \n",
    "the numpy interface (just as if it would be all in mem) - but it's backed by a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4a2d3ff-c810-4239-b5d3-e3364a2e1ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_filepath = \"./numpy.memmap\"\n",
    "db = np.memmap(db_filepath, mode='w+', dtype=np.float32, shape = (max_mems,2,dim))\n",
    "\n",
    "# db[1] = np.random.rand(1,2,dim) <<<<<< store 2 vectors in position 1 (second position)\n",
    "# db\n",
    "# memmap([[[0.        , 0.        , 0.        , ..., 0.        ,\n",
    "#          0.        , 0.        ],\n",
    "#         [0.        , 0.        , 0.        , ..., 0.        ,\n",
    "#          0.        , 0.        ]],\n",
    "#\n",
    "#        [[0.7254081 , 0.07150084, 0.92323035, ..., 0.28860936,\n",
    "#          0.20353778, 0.72763807],\n",
    "#         [0.00908635, 0.2388858 , 0.34646714, ..., 0.73859435,\n",
    "#          0.35705045, 0.8246553 ]],\n",
    "#\n",
    "#        [[0.        , 0.        , 0.        , ..., 0.        ,\n",
    "#          0.        , 0.        ],\n",
    "#         [0.        , 0.        , 0.        , ..., 0.        ,\n",
    "#          0.        , 0.        ]],\n",
    "#\n",
    "#        ...,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288bee68-1db2-4694-be99-77b1903323f5",
   "metadata": {},
   "source": [
    "Now we put this all in a class - object must be able to:\n",
    "- add to index and database\n",
    "- query the index\n",
    "- retrieve in database based on query results\n",
    "- remove from index and database (\"forgetting the oldest memories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "028c21aa-efa9-4d77-9ebf-3c680c8dc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDING\n",
    "\n",
    "# Let's first create some fake data that we can push through - and to understand requirements as well\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "keys = np.random.rand(batch_size, 512, dim).astype('float32')    # 512 is seq_len\n",
    "\n",
    "# Now we want to feed in the values as well, so we create an extra dimension\n",
    "\n",
    "keys_and_values = np.random.rand(batch_size, 512, 2, dim).astype('float32')\n",
    "\n",
    "# Say we have 16 as batch_size => 16 * 512 vector PAIRS (key and value) to into the db, while 16*512 key vectors go into the index\n",
    "# Note: this means we're storing a memory for each of the tokens in the sequence - but we only really use the last token to predict the\n",
    "# next one ... is this right?\n",
    "\n",
    "# Let's slam batch_size and seq_len together:\n",
    "\n",
    "keys_and_values = keys_and_values.reshape(-1, 2, dim)         # -1 means infer the first dimension\n",
    "\n",
    "# Now for the index we just need to store the keys, so let's grab those\n",
    "\n",
    "keys = keys_and_values[:,0,:]\n",
    "# Btw:\n",
    "# keys.shape\n",
    "# (512, 64)\n",
    "\n",
    "index.add(keys)\n",
    "# If you ever get error here on non contiguous array: index.add(np.ascontiguousarray(keys))\n",
    "\n",
    "# For the db numpy array we have to keep track of where we are ourselves\n",
    "\n",
    "db_offset = 0 # initialize once at the level of the object\n",
    "\n",
    "# Then each time the add function is called:\n",
    "\n",
    "num_added = keys_and_values.shape[0]\n",
    "\n",
    "indexes_added = (np.arange(num_added) + db_offset)\n",
    "\n",
    "db_offset = db_offset + num_added\n",
    "\n",
    "db[indexes_added] = keys_and_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbc74ed0-99b1-4147-9e54-f776e602dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERYING THE INDEX\n",
    "\n",
    "# Create fake vectors that we want to query with\n",
    "\n",
    "queries = np.random.rand(batch_size, 512, dim).astype('float32')    # We're going to query with EVERY token from every sequence in the batch \n",
    "\n",
    "# Again we slam batch_size and seq_len together\n",
    "queries = queries.reshape(-1, dim)\n",
    "\n",
    "# Let's try a search\n",
    "\n",
    "distances, ids = index.search(queries, top_k)\n",
    "\n",
    "# Since we store things in the db in the same order as for the index, we can simply query the db using the returned ids (which are indexes).\n",
    "\n",
    "result_keys_and_values = db[ids]\n",
    "\n",
    "\n",
    "#result_keys_and_values.shape # -> (1024, 2, 2, 64) for batch_size x seq_len, top_k, and then the query and the key vector\n",
    "\n",
    "# Now when we return this to the layer, we want to restore the batch and seq_len dimension.\n",
    "\n",
    "result_keys_and_values = result_keys_and_values.reshape(batch_size, 512, top_k, 2, dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85fee11-7431-4673-a75e-8b09a5d954ca",
   "metadata": {},
   "source": [
    "REMOVAL (forgetting)\n",
    "\n",
    "In the paper they have one memory for each batch dimension (the dimension in which a paper is fed in). So if batch_size is 8 then\n",
    "there are 8 memories (index and DB). This makes the whole thing more efficient since the best hits will come from the same paper\n",
    "anyway. In this implementation we'll just do a shared memory across all batches.\n",
    "\n",
    "Also in our implementation we're going to clear the database after each batch. In the batch dimension there are 1O chunks or 512\n",
    "tokens and we'll use the memory as we work through those 10, but after that we start with an empty memory again.\n",
    "\n",
    "So we're implementing a clear() function that simply clears out the index and the DB.\n",
    "\n",
    "So there's no real \"forgetting\" here, it's more like restarting from scratch with each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b397f7b1-054f-46f2-9616-bb82cd2e8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we need to add all of the above in a class form."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

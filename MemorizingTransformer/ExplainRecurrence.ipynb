{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34448184-6ce2-4b14-83a6-53182b8156c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat, pack, unpack, einsum\n",
    "\n",
    "b_s = 16\n",
    "seq_len = 512\n",
    "head_dim = 10\n",
    "num_attn_heads = 8\n",
    "embed_dim = 13 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3da1be2b-e17e-472a-a6e7-bcea8346d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some fake traininig data\n",
    "\n",
    "input_data = torch.randn(b_s, seq_len, embed_dim)\n",
    "\n",
    "Wq = nn.Linear(embed_dim, num_attn_heads * head_dim, bias=False)\n",
    "Wk = nn.Linear(embed_dim, num_attn_heads * head_dim, bias=False)\n",
    "Wv = nn.Linear(embed_dim, num_attn_heads * head_dim, bias=False)\n",
    "Wo = nn.Linear(num_attn_heads * head_dim, embed_dim) # Look up why Wo can hava a bias term\n",
    "\n",
    "queries = Wq(input_data)\n",
    "keys = Wk(input_data)\n",
    "values = Wv(input_data)\n",
    "\n",
    "# The above is all the same, now we're goint to add fake recurrence\n",
    "\n",
    "recurrence = torch.randn(b_s, seq_len, 2, num_attn_heads * head_dim) # This is how it will come in.\n",
    "\n",
    "recurrence_keys, recurrence_values = recurrence.unbind(dim=-2) # Each of the 2 has dim (b_s, seq_len, num_attn_heads * head_dim)\n",
    "\n",
    "xl_keys = torch.cat((recurrence_keys, keys), dim=-2) # Prepend the recurrence keys to the keys along the seq_len dimension (-2)\n",
    "# So we are basically doubling the seq_len\n",
    "xl_values = torch.cat((recurrence_values, values), dim=-2)\n",
    "\n",
    "#print(queries.shape)  # torch.Size([16, 512, 80])\n",
    "#print(xl_keys.shape)  # torch.Size([16, 1024, 80])\n",
    "#xl_values.shape       # torch.Size([16, 1024, 80])\n",
    "\n",
    "# Same as before: we pull the heads out and caculate attention scores per head with einsum.\n",
    "queries = rearrange(queries, 'b s (h d) -> b h s d', h = num_attn_heads)\n",
    "xl_keys = rearrange(xl_keys, 'b s (h d) -> b h s d', h = num_attn_heads) # s is twice as long here\n",
    "attn_scores = einsum(queries, xl_keys, 'b h s1 d, b h s2 d -> b h s1 s2')\n",
    "\n",
    "# Apply causal masking -> tokens from the recurrent matrix are all allowed to be seen.\n",
    "# 1    1    1    1    1 -inf -inf -inf\n",
    "# 1    1    1    1    1    1 -inf -inf\n",
    "# 1    1    1    1    1    1    1 -inf\n",
    "# 1    1    1    1    1    1    1    1\n",
    "\n",
    "rows, cols = attn_scores.shape[-2:]\n",
    "mask = torch.ones((rows, cols), dtype=torch.bool).triu(diagonal=(cols-rows)+1)\n",
    "attn_scores = attn_scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "# -1 here means that we sum up to 1 across the rows, see below\n",
    "attn_weights = F.softmax(attn_scores, dim = -1)\n",
    "\n",
    "# All the rest is reqular self attention again i.e. we rearrange the \n",
    "\n",
    "xl_values = rearrange(xl_values, 'b s (h d) -> b h s d', h = num_attn_heads) # s is twice as long here\n",
    "\n",
    "# print(attn_weights.shape) -> torch.Size([16, 8, 512, 1024])\n",
    "# print(xl_values.shape) -> torch.Size([16, 8, 1024, 10])\n",
    "\n",
    "head_context_vectors = attn_weights@xl_values\n",
    "# print(head_context_vectors.shape) -> torch.Size([16, 8, 512, 10])\n",
    "\n",
    "multihead_context_vector = rearrange(head_context_vectors, 'b h s d -> b s (h d)')\n",
    "#multihead_context_vector.shape -> torch.Size([16, 512, 80])\n",
    "\n",
    "out = Wo(multihead_context_vector)  # Back to embedding dimension\n",
    "# out.shape -> torch.Size([16, 512, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6674cb6-cd35-4ff2-9d54-e1294d5c93d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False]])\n",
      "tensor([[0., 1., 2., 3., 4., -inf, -inf, -inf],\n",
      "        [0., 1., 2., 3., 4., 5., -inf, -inf],\n",
      "        [0., 1., 2., 3., 4., 5., 6., -inf],\n",
      "        [0., 1., 2., 3., 4., 5., 6., 7.]])\n"
     ]
    }
   ],
   "source": [
    "# Intermezzo on triu\n",
    "\n",
    "import torch\n",
    "\n",
    "attn_scores = torch.tensor([[0.,1.,2.,3.,4.,5.,6.,7.],\n",
    "               [0.,1.,2.,3.,4.,5.,6.,7.],\n",
    "               [0.,1.,2.,3.,4.,5.,6.,7.],\n",
    "               [0.,1.,2.,3.,4.,5.,6.,7.]])\n",
    "\n",
    "rows, cols = attn_scores.shape[-2:] # Here is equiv. to attn_scores.shape[0:] -> basically we are getting the dims \n",
    "#print(attn_scores.shape) -> torch.Size([4, 8])\n",
    "\n",
    "mask = torch.ones((4, 8), dtype=torch.bool).triu(diagonal=(cols-rows)+1) # triu retains upper half, sets bottom to zero. Half is shifted up +4.\n",
    "# Note: if the inputs are square, then you can put \"rows+1\", but this would only work with recurrence.\n",
    "# Without recurrence we need the dim to be 1\n",
    "# So we use (cols-rows)+1 -> this is rows+1 if they are not the same and 1 otherwise.\n",
    "print(mask)\n",
    "attn_scores = attn_scores.masked_fill(mask, float(\"-inf\")) # Put -inf where mask is True, leave untouched elsewhere.\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac8a0ab8-4e41-4fa5-bcd5-8dc3208475ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.7661e-04, 1.5674e-03, 4.2606e-03, 1.1582e-02, 3.1482e-02, 8.5577e-02,\n",
      "         2.3262e-01, 6.3233e-01],\n",
      "        [3.9753e-31, 8.7561e-27, 1.9287e-22, 4.2482e-18, 9.3572e-14, 2.0611e-09,\n",
      "         4.5398e-05, 9.9995e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.7835e-44, 1.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00]])\n",
      "torch.Size([4, 8])\n",
      "8\n",
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Intermezzo on softmax\n",
    "\n",
    "attn_scores = torch.tensor([[0.,1.,2.,3.,4.,5.,6.,7.],\n",
    "               [0.,10.,20.,30.,40.,50.,60.,70.],\n",
    "               [0.,100.,200.,300.,400.,500.,600.,700.],\n",
    "               [0.,1000.,2000.,3000.,4000.,5000.,6000.,7000.]])\n",
    "attn_weights = F.softmax(attn_scores, dim = -1)\n",
    "print(attn_weights)\n",
    "print(attn_weights.shape) \n",
    "print(attn_weights.shape[-1])\n",
    "print(torch.sum(attn_weights, dim=-1))\n",
    "# attn_weights[0].sum() -> this sums up only the first row, sum() comes instead of the last dimension\n",
    "\n",
    "\n",
    "# dim -1 is the \"most inner\" list, so the row in the below. So after softmax, the row sums up to 1.\n",
    "#tensor([[5.7661e-04, 1.5674e-03, 4.2606e-03, 1.1582e-02, 3.1482e-02, 8.5577e-02,\n",
    "#         2.3262e-01, 6.3233e-01],\n",
    "#        [3.9753e-31, 8.7561e-27, 1.9287e-22, 4.2482e-18, 9.3572e-14, 2.0611e-09,\n",
    "#         4.5398e-05, 9.9995e-01],\n",
    "#        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "#         3.7835e-44, 1.0000e+00],\n",
    "#        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "#         0.0000e+00, 1.0000e+00]])\n",
    "#torch.Size([4, 8])\n",
    "#8\n",
    "#tensor([1., 1., 1., 1.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a942153-f29d-4978-a405-6e3ec62a07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now add the principles above to our existing MHSelfAttn, we take the einsum version here from ExplainEinsum as base (exact copy)\n",
    "\n",
    "class MHSelfAttn(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_attn_heads=8, head_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Need these in forward()\n",
    "        self.num_attn_heads = num_attn_heads\n",
    "        \n",
    "        self.Wq = nn.Linear(embed_dim, num_attn_heads * head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(embed_dim, num_attn_heads * head_dim, bias=False)\n",
    "        self.Wv = nn.Linear(embed_dim, num_attn_heads * head_dim, bias=False)\n",
    "        self.Wo = nn.Linear(num_attn_heads * head_dim, embed_dim) # Look up why Wo can hava a bias term\n",
    "\n",
    "#    def forward(self, input_data):\n",
    "    def forward(self, input_data, recurrence=None):\n",
    "\n",
    "        b_s, seq_len = input_data.shape[:2]\n",
    "        \n",
    "        queries = self.Wq(input_data)\n",
    "        keys = self.Wk(input_data)\n",
    "        values = self.Wv(input_data)\n",
    "\n",
    "        # Start new\n",
    "        if recurrence is not None:\n",
    "            # Copied from above:\n",
    "            recurrence_keys, recurrence_values = recurrence.unbind(dim=-2)\n",
    "            keys = torch.cat((recurrence_keys, keys), dim=-2)                 # xl_keys   -> keys so that rearrange below keeps working\n",
    "            values = torch.cat((recurrence_values, values), dim=-2)           # xl_values -> values same reason\n",
    "            # Note: for the first sequence, the keys and values stay the same, and we perform regular self attention.\n",
    "            recurrence_length = recurrence_keys.shape[1]                      # 2nd dim so the seq_len of just the recurrence\n",
    "        # End new\n",
    "        \n",
    "        queries = rearrange(queries, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "        keys = rearrange(keys, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "        values = rearrange(values, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "\n",
    "        attn_scores = einsum(queries, keys, 'b h s1 d, b h s2 d -> b h s1 s2')\n",
    "\n",
    "        i, j = attn_scores.shape[-2:]\n",
    "        mask = torch.ones((i,j), dtype=torch.bool).triu(diagonal=(j-i+1))\n",
    "        masked_attn_scores = attn_scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "        attn_weights = F.softmax(masked_attn_scores, dim = -1)\n",
    "\n",
    "        head_context_vectors = attn_weights@values\n",
    "        \n",
    "        multihead_context_vector = rearrange(head_context_vectors, 'b h s d -> b s (h d)')\n",
    "        out = self.Wo(multihead_context_vector)\n",
    "\n",
    "        # Start new\n",
    "        # New stuff to return the recurrence -> we only return the current keys and values because they have to \"trail\".\n",
    "        # 1. Restore the original arragement, note that keys and values are rectangular if recurrence was concat'ed\n",
    "        keys = rearrange(keys, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "        values = rearrange(values, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "        # Do the inverse of the unbind:\n",
    "        k_v_stacked = torch.stack((keys, values), dim=-2) # (b_s, seq_len, 2, embed_dim)\n",
    "\n",
    "        if recurrence is not None:\n",
    "            # Now we want to cut out only the currenct keys and values (I guess we can also store them before the concat)\n",
    "\n",
    "            # Recurrence is prepended\n",
    "            recurrence, current = k_v_stacked[:, :-recurrence_length], k_v_stacked[:, -recurrence_length:]\n",
    "\n",
    "            # >>> test\n",
    "            # tensor([[1, 2, 3, 4],\n",
    "            #         [5, 6, 7, 8]])\n",
    "            # >>> test[:,:-2]  -> everything until position -2\n",
    "            # tensor([[1, 2],\n",
    "            #         [5, 6]]) \n",
    "            # >>> test[:,-2:]  -> everything from position -2 on\n",
    "            # tensor([[3, 4],\n",
    "            #         [7, 8]])\n",
    "\n",
    "        else: # This means we processing the first sequence, which means we didn't concat so we can just pass on the keys/values\n",
    "            current = k_v_stacked\n",
    "        # End new\n",
    "        \n",
    "        # Add current\n",
    "        return out, current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3167e4-867f-4715-b81b-bf982a061b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do the same but for MHSelfAttnWithMem - additions marked with comments.\n",
    "\n",
    "\n",
    "class MHSelfAttnWithMem(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_attn_heads=8, head_dim=32, top_k=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_attn_heads = num_attn_heads\n",
    "        self.top_k = top_k\n",
    "\n",
    "        self.Wq = nn.Linear(embed_dim, num_attn_heads * head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(embed_dim, num_attn_heads * head_dim, bias=False)\n",
    "        self.Wv = nn.Linear(embed_dim, num_attn_heads * head_dim, bias=False)\n",
    "        self.Wo = nn.Linear(num_attn_heads * head_dim, embed_dim) # Look up why Wo can hava a bias term\n",
    "\n",
    "        self.gate = nn.Parameter(torch.randn(num_attn_heads, 1, 1))\n",
    "\n",
    "#    def forward(self, input_data, memory):\n",
    "    def forward(self, input_data, memory, recurrence):\n",
    "        b_s, seq_len = input_data.shape[:2]\n",
    "        \n",
    "        queries = self.Wq(input_data)\n",
    "        keys = self.Wk(input_data)\n",
    "        values = self.Wv(input_data)\n",
    "\n",
    "        # Start new - copied from MHSelfAttn above\n",
    "        if recurrence is not None:\n",
    "            recurrence_keys, recurrence_values = recurrence.unbind(dim=-2)\n",
    "            keys = torch.cat((recurrence_keys, keys), dim=-2)                \n",
    "            values = torch.cat((recurrence_values, values), dim=-2)\n",
    "            recurrence_length = recurrence_keys.shape[1]\n",
    "        # End new\n",
    "        \n",
    "        queries = rearrange(queries, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "        keys = rearrange(keys, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "        values = rearrange(values, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "\n",
    "        attn_scores = einsum(queries, keys, 'b h s1 d, b h s2 d -> b h s1 s2')\n",
    "\n",
    "        i, j = attn_scores.shape[-2:]\n",
    "        mask = torch.ones((i,j), dtype=torch.bool).triu(diagonal=(j-i+1))\n",
    "        masked_attn_scores = attn_scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "        attn_weights = F.softmax(masked_attn_scores, dim = -1)\n",
    "\n",
    "        head_context_vectors = attn_weights@values\n",
    "        \n",
    "        queries = rearrange(queries, 'b h s d -> b s (h d)')\n",
    "\n",
    "        mem_keys_and_values = memory.query(queries, self.top_k)\n",
    "\n",
    "        mem_keys, mem_values = mem_keys_and_values.unbind(-2)\n",
    "        \n",
    "        mem_keys = rearrange(mem_keys, 'b s k (h d) -> b h s k d', h = num_attn_heads)\n",
    "        mem_values = rearrange(mem_values, 'b s k (h d) -> b h s k d', h = num_attn_heads)\n",
    "\n",
    "        queries = rearrange(queries, 'b s (h d) -> b h s d', h=num_attn_heads)\n",
    "        \n",
    "        attn_scores = einsum(queries, mem_keys, 'b h s d, b h s k d -> b h s k')       \n",
    "        attn_scores = attn_scores * (head_dim ** -0.5)\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim = -1)\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weigths)\n",
    "\n",
    "        mem_context_vectors = einsum(attn_weights, mem_values, 'b h s k, b h s k d -> b h s d')\n",
    "\n",
    "        head_mem_context_vectors = (mem_context_vectors * self.gate) + (head_context_vectors * (1 - self.gate))\n",
    "        \n",
    "        multihead_context_vector = rearrange(head_mem_context_vector, 'b h s d -> b s (h d)')\n",
    "        out = self.Wo(multihead_context_vector)\n",
    "\n",
    "        # Start new - copied from above's MHSelfAttn\n",
    "        keys = rearrange(keys, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "        values = rearrange(values, 'b_s seq_len (num_attn_heads head_dim) -> b_s num_attn_heads seq_len head_dim', num_attn_heads=self.num_attn_heads)\n",
    "        k_v_stacked = torch.stack((keys, values), dim=-2) # (b_s, seq_len, 2, embed_dim)\n",
    "        if recurrence is not None:\n",
    "            recurrence, current = k_v_stacked[:, :-recurrence_length], k_v_stacked[:, -recurrence_length:]\n",
    "        else: \n",
    "            current = k_v_stacked\n",
    "        # End new\n",
    "\n",
    "        # ALSO STORE THE CURRENT keys and values in the knn memory (we didn't do this yet):\n",
    "        knn.add(current)\n",
    "\n",
    "        # Add current\n",
    "        return out, current\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

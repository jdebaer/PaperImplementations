{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34448184-6ce2-4b14-83a6-53182b8156c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat, pack, unpack, einsum\n",
    "\n",
    "b_s = 16\n",
    "seq_len = 512\n",
    "head_dim = 10\n",
    "num_attn_heads = 8\n",
    "embed_dim = 13 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3da1be2b-e17e-472a-a6e7-bcea8346d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some fake traininig data\n",
    "\n",
    "input_data = torch.randn(b_s, seq_len, embed_dim)\n",
    "\n",
    "Wq = nn.Linear(embed_dim, num_attn_heads * head_dim, bias=False)\n",
    "Wk = nn.Linear(embed_dim, num_attn_heads * head_dim, bias=False)\n",
    "Wv = nn.Linear(embed_dim, num_attn_heads * head_dim, bias=False)\n",
    "Wo = nn.Linear(num_attn_heads * head_dim, embed_dim) # Look up why Wo can hava a bias term\n",
    "\n",
    "queries = Wq(input_data)\n",
    "keys = Wk(input_data)\n",
    "values = Wv(input_data)\n",
    "\n",
    "# The above is all the same, now we're goint to add fake recurrence\n",
    "\n",
    "recurrence = torch.randn(b_s, seq_len, 2, num_attn_heads * head_dim) # This is how it will come in.\n",
    "\n",
    "recurrence_keys, recurrence_values = recurrence.unbind(dim=-2) # Each of the 2 has dim (b_s, seq_len, num_attn_heads * head_dim)\n",
    "\n",
    "xl_keys = torch.cat((recurrence_keys, keys), dim=-2) # Prepend the recurrence keys to the keys along the seq_len dimension (-2)\n",
    "# So we are basically doubling the seq_len\n",
    "xl_values = torch.cat((recurrence_values, values), dim=-2)\n",
    "\n",
    "#print(queries.shape)  # torch.Size([16, 512, 80])\n",
    "#print(xl_keys.shape)  # torch.Size([16, 1024, 80])\n",
    "#xl_values.shape       # torch.Size([16, 1024, 80])\n",
    "\n",
    "# Same as before: we pull the heads out and caculate attention scores per head with einsum.\n",
    "queries = rearrange(queries, 'b s (h d) -> b h s d', h = num_attn_heads)\n",
    "xl_keys = rearrange(xl_keys, 'b s (h d) -> b h s d', h = num_attn_heads) # s is twice as long here\n",
    "attn_scores = einsum(queries, xl_keys, 'b h s1 d, b h s2 d -> b h s1 s2')\n",
    "\n",
    "# Apply causal masking -> tokens from the recurrent matrix are all allowed to be seen.\n",
    "# 1    1    1    1    1 -inf -inf -inf\n",
    "# 1    1    1    1    1    1 -inf -inf\n",
    "# 1    1    1    1    1    1    1 -inf\n",
    "# 1    1    1    1    1    1    1    1\n",
    "\n",
    "rows, cols = attn_scores.shape[-2:]\n",
    "mask = torch.ones((rows, cols), dtype=torch.bool).triu(diagonal=(cols-rows)+1)\n",
    "attn_scores = attn_scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "# -1 here means that we sum up to 1 across the rows, see below\n",
    "attn_weights = F.softmax(attn_scores, dim = -1)\n",
    "\n",
    "# All the rest is reqular self attention again i.e. we rearrange the \n",
    "\n",
    "xl_values = rearrange(xl_values, 'b s (h d) -> b h s d', h = num_attn_heads) # s is twice as long here\n",
    "\n",
    "# print(attn_weights.shape) -> torch.Size([16, 8, 512, 1024])\n",
    "# print(xl_values.shape) -> torch.Size([16, 8, 1024, 10])\n",
    "\n",
    "head_context_vectors = attn_weights@xl_values\n",
    "# print(head_context_vectors.shape) -> torch.Size([16, 8, 512, 10])\n",
    "\n",
    "multihead_context_vector = rearrange(head_context_vectors, 'b h s d -> b s (h d)')\n",
    "#multihead_context_vector.shape -> torch.Size([16, 512, 80])\n",
    "\n",
    "out = Wo(multihead_context_vector)  # Back to embedding dimension\n",
    "# out.shape -> torch.Size([16, 512, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6674cb6-cd35-4ff2-9d54-e1294d5c93d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False]])\n",
      "tensor([[0., 1., 2., 3., 4., -inf, -inf, -inf],\n",
      "        [0., 1., 2., 3., 4., 5., -inf, -inf],\n",
      "        [0., 1., 2., 3., 4., 5., 6., -inf],\n",
      "        [0., 1., 2., 3., 4., 5., 6., 7.]])\n"
     ]
    }
   ],
   "source": [
    "# Intermezzo on triu\n",
    "\n",
    "import torch\n",
    "\n",
    "attn_scores = torch.tensor([[0.,1.,2.,3.,4.,5.,6.,7.],\n",
    "               [0.,1.,2.,3.,4.,5.,6.,7.],\n",
    "               [0.,1.,2.,3.,4.,5.,6.,7.],\n",
    "               [0.,1.,2.,3.,4.,5.,6.,7.]])\n",
    "\n",
    "rows, cols = attn_scores.shape[-2:] # Here is equiv. to attn_scores.shape[0:] -> basically we are getting the dims \n",
    "#print(attn_scores.shape) -> torch.Size([4, 8])\n",
    "\n",
    "mask = torch.ones((4, 8), dtype=torch.bool).triu(diagonal=(cols-rows)+1) # triu retains upper half, sets bottom to zero. Half is shifted up +4.\n",
    "# Note: if the inputs are square, then you can put \"rows+1\", but this would only work with recurrence.\n",
    "# Without recurrence we need the dim to be 1\n",
    "# So we use (cols-rows)+1 -> this is rows+1 if they are not the same and 1 otherwise.\n",
    "print(mask)\n",
    "attn_scores = attn_scores.masked_fill(mask, float(\"-inf\")) # Put -inf where mask is True, leave untouched elsewhere.\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac8a0ab8-4e41-4fa5-bcd5-8dc3208475ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.7661e-04, 1.5674e-03, 4.2606e-03, 1.1582e-02, 3.1482e-02, 8.5577e-02,\n",
      "         2.3262e-01, 6.3233e-01],\n",
      "        [3.9753e-31, 8.7561e-27, 1.9287e-22, 4.2482e-18, 9.3572e-14, 2.0611e-09,\n",
      "         4.5398e-05, 9.9995e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.7835e-44, 1.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00]])\n",
      "torch.Size([4, 8])\n",
      "8\n",
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Intermezzo on softmax\n",
    "\n",
    "attn_scores = torch.tensor([[0.,1.,2.,3.,4.,5.,6.,7.],\n",
    "               [0.,10.,20.,30.,40.,50.,60.,70.],\n",
    "               [0.,100.,200.,300.,400.,500.,600.,700.],\n",
    "               [0.,1000.,2000.,3000.,4000.,5000.,6000.,7000.]])\n",
    "attn_weights = F.softmax(attn_scores, dim = -1)\n",
    "print(attn_weights)\n",
    "print(attn_weights.shape) \n",
    "print(attn_weights.shape[-1])\n",
    "print(torch.sum(attn_weights, dim=-1))\n",
    "# attn_weights[0].sum() -> this sums up only the first row, sum() comes instead of the last dimension\n",
    "\n",
    "\n",
    "# dim -1 is the \"most inner\" list, so the row in the below. So after softmax, the row sums up to 1.\n",
    "#tensor([[5.7661e-04, 1.5674e-03, 4.2606e-03, 1.1582e-02, 3.1482e-02, 8.5577e-02,\n",
    "#         2.3262e-01, 6.3233e-01],\n",
    "#        [3.9753e-31, 8.7561e-27, 1.9287e-22, 4.2482e-18, 9.3572e-14, 2.0611e-09,\n",
    "#         4.5398e-05, 9.9995e-01],\n",
    "#        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "#         3.7835e-44, 1.0000e+00],\n",
    "#        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
    "#         0.0000e+00, 1.0000e+00]])\n",
    "#torch.Size([4, 8])\n",
    "#8\n",
    "#tensor([1., 1., 1., 1.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a942153-f29d-4978-a405-6e3ec62a07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24:00 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
